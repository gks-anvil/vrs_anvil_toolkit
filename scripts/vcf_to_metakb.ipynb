{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VRS workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seqrepo ga4gh.vrs[extras]==2.0.0a2 ga4gh.vrs\n",
    "%pip install --upgrade --no-cache-dir terra-notebook-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ga4gh.vrs.extras.vcf_annotation import VCFAnnotator\n",
    "from pathlib import Path\n",
    "from terra_notebook_utils import drs, table\n",
    "\n",
    "import datetime\n",
    "import multiprocessing\n",
    "import os \n",
    "import pickle\n",
    "import requests\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store relevant variables\n",
    "\n",
    "%env SEQREPO_ROOT=/home/jupyter/seqrepo\n",
    "%env VCFTOOLS_DIR=/home/jupyter/vcftools\n",
    "%env PERL5LIB=/home/jupyter/vcftools/src/perl/\n",
    "%env VCFTOOLS=/home/jupyter/vcftools/src/cpp/vcftools\n",
    "%env OUTPUT=/home/jupyter/output\n",
    "%env SPLIT_DIR=/home/jupyter/split\n",
    "!mkdir $SPLIT_DIR\n",
    "!mkdir $OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash ~/setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get VCFs ([GREGoR](https://anvil.terra.bio/#workspaces/anvil-datastorage/AnVIL_GREGoR_UW_CRDR_U05_GRU/data); [1000G](https://explore.anvilproject.org/files?filter=%5B%7B%22categoryKey%22%3A%22files.file_format%22%2C%22value%22%3A%5B%22.vcf%22%5D%7D%5D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir ~/vcf\n",
    "\n",
    "# # get gatk_exome (local only) TODO\n",
    "# os.environ[\"GATK_VCF_FILE\"] = \"gatk_exome.vcf\"\n",
    "# os.environ[\"GATK_VCF\"] = \"gs://fc-secure-ff61133c-8d50-49c3-84e1-a93489621c7f/gatk_exome.vcf\"\n",
    "# # TODO: check if already exists\n",
    "# ! gsutil -u $GOOGLE_PROJECT cp $GATK_VCF \"~/vcf/$GATK_VCF_FILE\"\n",
    "\n",
    "# # for GREGoR\n",
    "# ! gsutil -u $GOOGLE_PROJECT cp 'gs://fc-secure-9cc1ea56-e4bf-47fe-9229-40d4d7f452bf/1369747.merged.matefixed.sorted.markeddups.recal.g.vcf.gz' ~/vcf/1369747.merged.matefixed.sorted.markeddups.recal.g.vcf.gz\n",
    "# ! gunzip ~/vcf/1369747.merged.matefixed.sorted.markeddups.recal.g.vcf.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vcf_from_drs_uri(uri):\n",
    "    file_name = drs.info(uri)[\"name\"]\n",
    "    vcf_path = f\"/home/jupyter/vcf/{file_name}\"\n",
    "    \n",
    "    print(\"trying...\")\n",
    "    \n",
    "    if not os.path.exists(vcf_path):\n",
    "        drs.copy(uri, vcf_path)\n",
    "        print(f\"written to {vcf_path} \\n\")\n",
    "    else:\n",
    "        print(f\"file already exists at {vcf_path} \\n\")\n",
    "\n",
    "\n",
    "    return vcf_path\n",
    "\n",
    "drs_uris = [\n",
    "#             \"drs://drs.anv0:v2_1dc37127-8329-3ce5-b019-4ed290946fcb\", # 1000G_omni2.5.chrY.vcf\n",
    "#             \"drs://drs.anv0:v2_7f4758a6-7b61-3290-98a0-585e12af3997\", # 1000G_phase1.snps.high_confidence.hg38.chrY.vcf\n",
    "#             \"drs://drs.anv0:v2_f33f8839-b3e9-39e2-8318-a650b995cc45\", # HG002vCHM13_20200921_mm2_PBCCS_sniffles.s2l20.refined.nSVtypes.ism.vcf\n",
    "            \"drs://drs.anv0:v2_950a3291-674c-32e7-8466-7dfd40ddd7a8\", # HG006vCHM13_20200921_mm2_PBCCS_sniffles.s2l20.refined.nSVtypes.ism.vcf\n",
    "#             \"drs://drs.anv0:v2_497867a4-c4b4-3dc0-955b-4c638331e0aa\", # HG006vGRCh38_mm2_ONT_sniffles.s2l20.refined.nSVtypes.ism.vcf\n",
    "#             \"drs://drs.anv0:v2_16d0715f-d70f-3180-a022-f40e6f1d1ce4\", # HG02080vCHM13_20200921_mm2_ONT_sniffles.s2l20.refined.nSVtypes.ism.vcf\n",
    "#             \"drs://drs.anv0:v2_02513ada-a6bd-3be6-887f-f1f85ed4649f\", # Homo_sapiens_assembly38.dbsnp138.chrY.vcf\n",
    "#             \"drs://drs.anv0:v2_6f73895f-4d17-3a3d-a7bb-5f4e2f972f55\", # chm13_hifi_HG007.crossaligner.vcf\n",
    "#             \"drs://drs.anv0:v2_c6545454-c695-3e73-8b08-29757fca9fd5\", # grch38_HG002_trio_merged.vcf\n",
    "#            \"drs://drs.anv0:v2_4dd732ef-973b-354a-823a-c48b4c9f4a1f\" # grch38_hifi_HG006.crossaligner.vcf\n",
    "           ]\n",
    "drs_vcfs = [get_vcf_from_drs_uri(uri) for uri in drs_uris]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get file sizes\n",
    "\n",
    "def human_size(num_bytes):\n",
    "    suffixes = ['B', 'KB', 'MB', 'GB', 'TB', 'PB', 'EB', 'ZB', 'YB']\n",
    "    for suffix in suffixes:\n",
    "        if num_bytes < 1024:\n",
    "            return f\"{num_bytes:.2f} {suffix}\" \n",
    "        num_bytes /= 1024\n",
    "\n",
    "    return f\"{num_bytes:.2f} {suffixes[-1]}\"\n",
    "\n",
    "for uri in drs_uris:\n",
    "    info = drs.info(uri)\n",
    "    print(f\"{info['name']}: {human_size(info['size'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split before annotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $VCFTOOLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ~/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $VCFTOOLS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcf_path = drs_vcfs[0]\n",
    "# %env VCF_PATH = vcf_path\n",
    "\n",
    "# os.environ['VCF_PATH'] = \"~/vcf/\"+os.environ['GATK_VCF_FILE']\n",
    "\n",
    "# w/ chr prefix\n",
    "# os.environ[\"VCF_PATH\"] = vcf_path\n",
    "# ! (seq 1 22; echo X; echo Y) | xargs -P 0 -I PATH $VCFTOOLS --recode --vcf $VCF_PATH --chr PATH --out ~/split/chrPATH\n",
    "\n",
    "split_vcf_cmd = f\"(seq 1 22; echo X; echo Y) | \\\n",
    "               xargs -P 0 -I PATH $VCFTOOLS --recode --vcf {vcf_path} \\\n",
    "               --chr chrPATH --out $SPLIT_DIR/chrPATH\"\n",
    "\n",
    "subprocess.run(split_vcf_cmd, shell=True, check=True) \n",
    "\n",
    "# no chr prefix\n",
    "# ! (seq 1 22; echo X; echo Y) | xargs -P 0 -I PATH ~/vcftools-vcftools-d511f46/src/cpp/vcftools --recode --vcf $VCF_PATH --chr PATH --out ~/split/chrPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: parse logs to get outputs on how many were filtered out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -l $SPLIT_DIR/*.recode.vcf | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate each of them\n",
    "# TODO: fix the outputs coming from this\n",
    "\n",
    "! ls -1 $SPLIT_DIR/*.recode.vcf |  xargs -P 0 -I PATH python3 -m ga4gh.vrs.extras.vcf_annotation --vcf_in PATH --vcf_out PATH.vcf.gz --vrs_pickle_out PATH.pkl --seqrepo_root_dir ~/seqrepo/latest/\n",
    "\n",
    "# # GREGoR\n",
    "# !python3 -m ga4gh.vrs.extras.vcf_annotation --vcf_in 1369747.merged.matefixed.sorted.markeddups.recal.g.vcf  --vcf_out 1369747.merged.matefixed.sorted.markeddups.recal.g.vcf.output.vcf.gz --vrs_pickle_out 1369747.merged.matefixed.sorted.markeddups.recal.g.vcf.vrs_objects.pkl  --seqrepo_root_dir ~/seqrepo/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l $SPLIT_DIR/*.vcf.vcf.gz | wc -l\n",
    "!ls -l $SPLIT_DIR/*.vcf.pkl | wc -l\n",
    "\n",
    "# assert (!ls -l ~/split/*.vcf.vcf.gz | wc -l) == 24, \"incorrect number of output vcf.gz files created\"\n",
    "# assert (!ls -l ~/split/*.vcf.pkl | wc -l) == 24, \"incorrect number of outputted pickle files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the files\n",
    "!ls -1 $SPLIT_DIR/*.vcf.vcf.gz | xargs $PERL5LIB/vcf-concat > $OUTPUT/merged_output.vcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ~/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: remove the pair of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Random python annotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"ga4gh.vrs.extras.vcf_annotation\")\n",
    "logger.setLevel(level=logging.INFO)\n",
    "\n",
    "# create annotated vcf test file \n",
    "def annotate_vcf(path):\n",
    "    '''param stem: path of input vcf file'''\n",
    "    stem = path.replace(\".vcf\", \"\")\n",
    "    \n",
    "    input_vcf = path\n",
    "    output_vcf = f\"{stem}.output.vcf.gz\"\n",
    "    output_pkl = f\"{stem}-vrs-objects.pkl\"\n",
    "\n",
    "    \n",
    "    vcf_annotator = VCFAnnotator(seqrepo_root_dir=\"/home/jupyter/seqrepo/latest\")\n",
    "    vcf_annotator.annotate(vcf_in=input_vcf, vcf_out=output_vcf, vrs_pickle_out=output_pkl)\n",
    "    # vcf_annotator.annotate(vcf_in=input_vcf, vrs_pickle_out=output_pkl)\n",
    "    \n",
    "# annotate_vcf(\"/home/jupyter/split\", \"chr1.recode\")\n",
    "successes = set()\n",
    "for vcf_path in drs_vcfs:\n",
    "    try:\n",
    "        print(\"trying...\", vcf_path)\n",
    "        annotate_vcf(vcf_path)\n",
    "        print(\"worked \\n\")\n",
    "        successes.add(vcf_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"unsucessful, see logs above \\n\")\n",
    "\n",
    "print(f\"total successes: {len(successes)}/{len(drs_vcfs)} \\nList...\")\n",
    "for vcf_path in drs_vcfs:\n",
    "    print(f\"{vcf_path}: {'✓' if vcf_path in successes else 'x'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# annotate w vrs id asking for output vcf\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"ga4gh.vrs.extras.vcf_annotation\")\n",
    "logger.setLevel(level=logging.INFO)\n",
    "\n",
    "# create annotated vcf test file \n",
    "def annotate_vcf(path):\n",
    "    '''param stem: path of input vcf file'''\n",
    "    stem = path.replace(\".vcf\", \"\")\n",
    "    \n",
    "    input_vcf = path\n",
    "    output_vcf = f\"{stem}.output.vcf.gz\"\n",
    "    output_pkl = f\"{stem}-vrs-objects.pkl\"\n",
    "\n",
    "    \n",
    "    vcf_annotator = VCFAnnotator(seqrepo_root_dir=\"/home/jupyter/seqrepo/latest\")\n",
    "    vcf_annotator.annotate(vcf_in=input_vcf, vcf_out=output_vcf, vrs_pickle_out=output_pkl)\n",
    "    # vcf_annotator.annotate(vcf_in=input_vcf, vrs_pickle_out=output_pkl)\n",
    "    \n",
    "# annotate_vcf(\"/home/jupyter/split\", \"chr1.recode\")\n",
    "successes = set()\n",
    "for vcf_path in drs_vcfs:\n",
    "    try:\n",
    "        print(\"trying...\", vcf_path)\n",
    "        annotate_vcf(vcf_path)\n",
    "        print(\"worked \\n\")\n",
    "        successes.add(vcf_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"unsucessful, see logs above \\n\")\n",
    "\n",
    "print(f\"total successes: {len(successes)}/{len(drs_vcfs)} \\nList...\")\n",
    "for vcf_path in drs_vcfs:\n",
    "    print(f\"{vcf_path}: {'✓' if vcf_path in successes else 'x'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for vcf_path in drs_vcfs:\n",
    "    if \"HG02080vCHM13_20200921\" in vcf_path:\n",
    "        print(vcf_path)\n",
    "    else:\n",
    "        continue\n",
    "#     if \"chm13_hifi_HG007\" in vcf_path:\n",
    "#         print(\"trying...\", vcf_path)\n",
    "#         annotate_vcf(vcf_path)\n",
    "#         print(\"worked \\n\")\n",
    "    try:\n",
    "        print(\"trying...\", vcf_path)\n",
    "        annotate_vcf(vcf_path)\n",
    "        print(\"worked \\n\")\n",
    "        successes.add(vcf_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"unsucessful, see logs above \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# annotate w vrs id only pickle outputted\n",
    "\n",
    "logger = logging.getLogger(\"ga4gh.vrs.extras.vcf_annotation\")\n",
    "# logger.setLevel(level=logging.ERROR)\n",
    "logger.disabled = True\n",
    "\n",
    "# create annotated vcf test file \n",
    "def annotate_vcf_pkl_only(path):\n",
    "    '''param stem: path of input vcf file'''\n",
    "    stem = path.replace(\".vcf\", \"\")\n",
    "    \n",
    "    input_vcf = path\n",
    "    output_vcf = f\"{stem}.output.vcf.gz\"\n",
    "    output_pkl = f\"{stem}-vrs-objects.pkl\"\n",
    "\n",
    "    \n",
    "    vcf_annotator = VCFAnnotator(seqrepo_root_dir=\"/home/jupyter/seqrepo/latest\")\n",
    "    vcf_annotator.annotate(vcf_in=input_vcf, vrs_pickle_out=output_pkl)\n",
    "    \n",
    "successes = set()\n",
    "for i, vcf_path in enumerate(drs_vcfs):\n",
    "    print(\"starting... \\n\")\n",
    "    # annotate to output pkl\n",
    "    try:\n",
    "        print(\"trying...\", vcf_path)\n",
    "        annotate_vcf_pkl_only(vcf_path)\n",
    "        print(\"worked \\n\")\n",
    "        successes.add(vcf_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"unsucessful, see logs above \\n\")\n",
    "    \n",
    "    # get pickle totals\n",
    "    try:\n",
    "        with open(output_pkl, 'rb') as f:\n",
    "            vrs_objects = pickle.load(f)\n",
    "\n",
    "        # get total num_variants\n",
    "        vcf_reader = vcf.Reader(open(vcf_path, 'r'))\n",
    "        num_variants = sum(1 for record in vcf_reader)\n",
    "\n",
    "        # view details\n",
    "        print(f'num_vrs_objects to num_varaints: {len(vrs_objects)}/{num_variants}={(len(vrs_objects)/num_variants):.2f}%')\n",
    "    except:\n",
    "        print(\"unable to get pickle totals, file may not exist\")\n",
    "    print()\n",
    "\n",
    "print(f\"total successes: {len(successes)}/{len(drs_vcfs)} \\nList...\")\n",
    "for vcf_path in drs_vcfs:\n",
    "    print(f\"{vcf_path}: {'✓' if vcf_path in successes else 'x'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import vcf\n",
    "\n",
    "\n",
    "# for input_vcf_file in [\"/home/jupyter/vcf/long_read_sv_jasmine_Trios_IndividualCallsets_CHM13_HG005_Trio_HG006vCHM13_20200921_mm2_PBCCS_sniffles.s2l20.refined.nSVtypes.ism.vcf\"]:\n",
    "for input_vcf_file in [\"/home/jupyter/vcf/long_read_minimap2_alignments_HG02080vCHM13_20200921_mm2_ONT_sniffles.s2l20.refined.nSVtypes.ism.vcf\"]:\n",
    "    output_vcf_file = \"/home/jupyter/vcf/long_read.test.vcf\"\n",
    "\n",
    "    vcf_reader = vcf.Reader(open(input_vcf_file, 'r'))\n",
    "    vcf_writer = vcf.Writer(open(output_vcf_file, 'w'), vcf_reader)\n",
    "\n",
    "    for record in vcf_reader:\n",
    "        record.INFO['VRS_ALLELE_ID'] = 'ga4gh:VA.xksahgfowdfdwofd,ga4gh:VA.xksahgfowdfdwofd'\n",
    "        vcf_writer.write_record(record)\n",
    "\n",
    "vcf_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show loaded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# import pickle\n",
    "# import ast\n",
    "# import requests\n",
    "# import datetime\n",
    "\n",
    "# # log progress\n",
    "# progress_interval = 50000\n",
    "\n",
    "# # load pickled dict\n",
    "# with open(output_pkl, 'rb') as f:\n",
    "#     print(datetime.datetime.now().isoformat(), 'opened pickle')\n",
    "#     vrs_objects = pickle.load(f)\n",
    "#     c = 0\n",
    "#     for k, v in vrs_objects.items():\n",
    "#         vrs_objects[k] = ast.literal_eval(v)\n",
    "#         c += 1\n",
    "#         if c % progress_interval == 0:\n",
    "#             print(datetime.datetime.now().isoformat(), c)\n",
    "\n",
    "# # view details        \n",
    "# print('number of vrs objects', len(vrs_objects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_paths = !ls -1 ~/split/*.vcf.pkl\n",
    "pickle_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get percent of loaded variants\n",
    "import vcf\n",
    "import pickle\n",
    "import ast\n",
    "\n",
    "# load pickled dict\n",
    "# for vcf_path in drs_vcfs:\n",
    "\n",
    "def unpickle_generator(file_name):\n",
    "    \"\"\"Unpickle vrs objects, yields (key,vrs_object)\"\"\"\n",
    "    with open(file_name, 'rb') as f:\n",
    "        vrs_objects = pickle.load(f)\n",
    "        for k, v in vrs_objects.items():\n",
    "            yield k, ast.literal_eval(v)\n",
    "            \n",
    "def unpickle(file_name):\n",
    "    \"\"\"Unpickle vrs objects to single dict\"\"\"\n",
    "    with open(file_name, 'rb') as f:\n",
    "        vrs_objects = pickle.load(f)\n",
    "        for k, v in vrs_objects.items():\n",
    "            vrs_objects[k] = ast.literal_eval(v)\n",
    "    \n",
    "    return vrs_objects\n",
    "\n",
    "vrs_dicts = []\n",
    "\n",
    "for path in pickle_paths:\n",
    "    vrs_dict = unpickle(path)\n",
    "    vrs_dicts.append(vrs_dict)\n",
    "\n",
    "    # get total num_variants\n",
    "    # TODO: reference the new merged file bc some might have been filtered out\n",
    "    vcf_reader = vcf.Reader(open(path[:-4], 'r'))\n",
    "    num_variants = sum(1 for record in vcf_reader)\n",
    "\n",
    "#     num_vrs_objs = sum((1 for _ in vrs_objects))\n",
    "    num_vrs_objs = len(vrs_dict)\n",
    "\n",
    "    # view details\n",
    "    \n",
    "    print(path.split(\"/\")[-1], end=\" \")\n",
    "    if num_variants == 0: \n",
    "        print(f\"no variants\") \n",
    "    else:\n",
    "        print(f'vrs_objects:variants = {num_vrs_objs}/{num_variants} = {(50*num_vrs_objs/num_variants):.1f}%')\n",
    "\n",
    "# TODO on combining: have to think about this more bc large files will have to be held in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysam\n",
    "\n",
    "for vcf_path in drs_vcfs:\n",
    "    # get pickle totals\n",
    "    output_pkl = vcf_path.replace(\".vcf\", \"\")+\"-vrs-objects.pkl\"\n",
    "    with open(output_pkl, 'rb') as f:\n",
    "        vrs_objects = pickle.load(f)\n",
    "\n",
    "    # get total num_variants\n",
    "    vcf_reader = pysam.VariantFile(open(vcf_path, 'r'))\n",
    "    num_variants = sum(1 for record in vcf_reader)\n",
    "    print(f'num_vrs_objects to num_variants: {len(vrs_objects)}/{num_variants}={(100*len(vrs_objects)/num_variants):.2f}%')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def unpickle(file_name):\n",
    "#     \"\"\"Unpickle vrs objects, yields (key,vrs_object)\"\"\"\n",
    "#     with open(file_name, 'rb') as f:\n",
    "#         vrs_objects = pickle.load(f)\n",
    "#         for k, v in vrs_objects.items():\n",
    "#             yield k, ast.literal_eval(v)\n",
    "            \n",
    "            \n",
    "            \n",
    "# chr1_vrs_objects = unpickle(output_pkl)\n",
    "# print('number of vrs objects', list(chr1_vrs_objects))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query remote services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MetaKB (cancervariants.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_kb(item: tuple):\n",
    "    \"\"\"Query metakb\"\"\"\n",
    "    k, _ = item\n",
    "    \n",
    "    response = requests.get(f\"http://metakb-dev-eb.us-east-2.elasticbeanstalk.com/api/v2/search/studies?variation={_['id']}&detail=false\")\n",
    "    response_json = response.json()\n",
    "    \n",
    "    if response_json['warnings'] == []:\n",
    "        summary = {}\n",
    "        summary['description'] = response_json['statements'][0]['description']\n",
    "        return (k, _['_id'], summary)\n",
    "\n",
    "def meta_kb_old(item: tuple):\n",
    "    \"\"\"Query metakb\"\"\"\n",
    "    k, _ = item\n",
    "    \n",
    "    response = requests.get(f\"https://dev-search.cancervariants.org/api/v2/search?variation={_['id']}&detail=false\")\n",
    "    response_json = response.json()\n",
    "    \n",
    "    if response_json['warnings'] == []:\n",
    "        summary = {}\n",
    "        summary['description'] = response_json['statements'][0]['description']\n",
    "        return (k, _['_id'], summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"elastic beanstalk link\")\n",
    "for vrs_dict in vrs_dicts:\n",
    "    hits = []\n",
    "    \n",
    "    for obj in vrs_dict.items():\n",
    "        potential_hit = meta_kb(obj)\n",
    "        if potential_hit:\n",
    "            hits.append(potential_hit)\n",
    "    \n",
    "    if len(vrs_dict) == 0:\n",
    "        continue\n",
    "\n",
    "    hit_rate = len(hits)/len(vrs_dict)    \n",
    "    print(f\"hit rate of VRS IDs: {len(hits)}/{len(vrs_dict)}={100*hit_rate:.1f}%\" )\n",
    "    if len(hits) > 0:\n",
    "        print(\"first few hits\")\n",
    "        print(hits[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old link\n",
    "\n",
    "for vrs_dict in vrs_dicts:\n",
    "    hits = []\n",
    "    \n",
    "    for obj in vrs_dict.items():\n",
    "        potential_hit = meta_kb_old(obj)\n",
    "        if potential_hit:\n",
    "            hits.append(potential_hit)\n",
    "    \n",
    "    if len(vrs_dict) == 0:\n",
    "        print(\"no vrs_ids\")\n",
    "        continue\n",
    "\n",
    "    hit_rate = len(hits)/len(vrs_dict)    \n",
    "    print(f\"hit rate of VRS IDs: {len(hits)}/{len(vrs_dict)}={100*hit_rate:.1f}%\" )\n",
    "    if len(hits) > 0:\n",
    "        print(\"first few hits\")\n",
    "        print(hits[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vrs_dict in vrs_dicts[:1]:\n",
    "    hits = []\n",
    "    \n",
    "    for i, (k, v) in enumerate(vrs_dict.items()):\n",
    "        print(v)\n",
    "        if i > 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_kb_by_sequence(item: tuple):\n",
    "    \"\"\"Query metakb\"\"\"\n",
    "    k, _ = item\n",
    "    \n",
    "\n",
    "    response = requests.get(f\"https://dev-search.cancervariants.org/api/v2/search?variation={}&detail=false\")\n",
    "    response_json = response.json()\n",
    "    \n",
    "    print(response_json['warnings'])\n",
    "    if response_json['warnings'] == []:\n",
    "        summary = {}\n",
    "        summary['description'] = response_json['statements'][0]['description']\n",
    "        return (k, _['_id'], summary)\n",
    "\n",
    "print(\"trying old link\")\n",
    "for vrs_dict in vrs_dicts:\n",
    "    hits = []\n",
    "    \n",
    "    for obj in vrs_dict.items():\n",
    "        potential_hit = meta_kb_by_sequence(obj)\n",
    "        if potential_hit:\n",
    "            hits.append(potential_hit)\n",
    "    \n",
    "    if len(vrs_dict) == 0:\n",
    "        print(\"no variants, skipping...\")\n",
    "        continue\n",
    "\n",
    "    hit_rate = len(hits)/len(vrs_dict)    \n",
    "    print(f\"hit rate of VRS IDs: {len(hits)}/{len(vrs_dict)}={100*hit_rate:.1f}%\" )\n",
    "    if len(hits) > 0:\n",
    "        print(\"first few hits\")\n",
    "        print(hits[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decorate(vrs_decorator, vrs_objects, limit=20):\n",
    "    \"\"\"harvest data from service\"\"\"\n",
    "\n",
    "    # log progress\n",
    "    progress_interval = 1000\n",
    "\n",
    "\n",
    "    # number of workers\n",
    "    worker_count = 12\n",
    "\n",
    "    with multiprocessing.Pool(worker_count) as pool:\n",
    "        # call the function for each item in parallel\n",
    "        c = 0\n",
    "        print(datetime.datetime.now().isoformat(), c)\n",
    "        for result in pool.imap(vrs_decorator, vrs_objects.items()):\n",
    "            c += 1\n",
    "            if result:\n",
    "                print(result[0], result[-1])\n",
    "            if c == limit:\n",
    "                break\n",
    "            if c % progress_interval == 0:\n",
    "                print(datetime.datetime.now().isoformat(), c)\n",
    "\n",
    "    print(datetime.datetime.now().isoformat(), c)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decorate(vrs_decorator=meta_kb, vrs_objects=metakb_vrs_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## ClinGen (clinicalgenome.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def clingen(item: tuple):\n",
    "    \"\"\"Query clingen (old version of normalizer)\"\"\"\n",
    "    k, _ = item\n",
    "    _id = _['_id'].split(':')[-1].split('.')[-1]\n",
    "    response = requests.get(f\"https://reg.genome.network/vrs-map/digest/vrs/{_id}\")\n",
    "    response_json = response.json()\n",
    "    if response_json['status']['code'] == 200:\n",
    "        iri_response = requests.get(response_json['data']['iri'])\n",
    "        iri_response_json = iri_response.json()\n",
    "        return (k, _['_id'], {'communityStandardTitle': iri_response_json['communityStandardTitle']})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note: at this time, there is a schema mismatch between vrs-python, metakb and clingen. We will use known identifiers. Normally the annotated identifiers from the variants of interest (vcf) would be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "decorate(vrs_decorator=clingen, vrs_objects=clingen_vrs_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
