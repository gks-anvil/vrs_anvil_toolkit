{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VCF -> VRS ID -> Clinical Evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install seqrepo ga4gh.vrs[extras]==2.0.0a3 ga4gh.vrs\n",
    "%pip install --upgrade --no-cache-dir terra-notebook-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from datetime import datetime\n",
    "from firecloud import api as fapi\n",
    "from ga4gh.core import ga4gh_identify\n",
    "from ga4gh.vrs import models\n",
    "from ga4gh.vrs.extras.vcf_annotation import VCFAnnotator\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from terra_notebook_utils import drs\n",
    "from time import time \n",
    "\n",
    "import ast\n",
    "import datetime\n",
    "import glob\n",
    "import io\n",
    "import json\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os \n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pysam\n",
    "import requests\n",
    "import subprocess\n",
    "import sys\n",
    "import vcf\n",
    "import zipfile\n",
    "\n",
    "# repo utils\n",
    "sys.path.append('../../vrs-python-testing')\n",
    "from utils import get_num_variants, metakb, print_dict, print_percent, truncate, unpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store relevant variables\n",
    "\n",
    "%env SEQREPO_ROOT=/home/jupyter/seqrepo\n",
    "%env VCFTOOLS_DIR=/home/jupyter/vcftools\n",
    "%env BCF_TOOLS_DIR=/home/jupyter/bcftools\n",
    "%env PERL5LIB=/home/jupyter/vcftools/src/perl/\n",
    "%env VCFTOOLS=/home/jupyter/vcftools/src/cpp/vcftools\n",
    "%env OUTPUT=/home/jupyter/output\n",
    "%env SPLIT_DIR=/home/jupyter/split\n",
    "%env INPUT_DIR=/home/jupyter/vcf\n",
    "!mkdir $INPUT_DIR\n",
    "!mkdir $SPLIT_DIR\n",
    "!mkdir $OUTPUT\n",
    "\n",
    "SEQREPO_DIR = os.environ[\"SEQREPO_ROOT\"]+\"/latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install vcftools and complete setup\n",
    "# don't worry about the pyvcf error \n",
    "\n",
    "!bash ~/vrs-python-testing/terra/setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get [1000G](https://anvil.terra.bio/#workspaces/anvil-datastorage/AnVIL_1000G_PRIMED-data-model/data) VCF Data for NA12878"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify patient and chromosomes\n",
    "\n",
    "chrs_of_interest = [str(num) for num in range(1,2)]\n",
    "chr_set = set(chrs_of_interest)\n",
    "patient = \"NA12878\"\n",
    "chrs_of_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metadata for filepaths\n",
    "# openly sourced from https://anvil.terra.bio/#workspaces/anvil-datastorage/AnVIL_1000G_PRIMED-data-model/data\n",
    "\n",
    "df = pd.read_csv(io.StringIO(fapi.get_entities_tsv(\"anvil-datastorage\", \\\n",
    "                \"AnVIL_1000G_PRIMED-data-model\", \"sequencing_file\", model=\"flexible\").text), sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of gvcf data\n",
    "\n",
    "df_vcf = df[df['file_type'].isin(['VCF', 'VCF index'])]\n",
    "df_1kgp = df_vcf[df_vcf['file_path'].str.contains('1kGP')]\n",
    "\n",
    "num_vcf_idx_files = sum(df_1kgp['file_type'] == 'VCF index')\n",
    "num_vcf_files = sum(df_1kgp['file_type'] == 'VCF')\n",
    "assert num_vcf_files == 23 and num_vcf_idx_files == 23, \\\n",
    "    f\"check number of files, {num_vcf_files} vcfs and {num_vcf_idx_files} index files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# load 1000G files if doesn't exist\n",
    "\n",
    "df_chrs = df_1kgp[df_1kgp['chromosome'].isin(chr_set)]\n",
    "assert(len(df_chrs) == 2*len(chr_set)), \\\n",
    "    f\"Expected 2 files per chr but {len(df_chrs)} files and {len(chr_set)} chrs\"\n",
    "\n",
    "uris = df_chrs['file_path']\n",
    "file_names = [uri.split(\"/\")[-1] for uri in uris]\n",
    "\n",
    "for file_name, uri in zip(file_names, uris):\n",
    "    if os.path.exists(f\"{os.environ['INPUT_DIR']}/{file_name}\"):\n",
    "        print(f\"{truncate(file_name, 35, 10)} already exists, not downloading\")\n",
    "    else:\n",
    "        split_vcf_cmd = f\"gsutil -u $GOOGLE_PROJECT cp {uri} $INPUT_DIR/\"\n",
    "        output = subprocess.run(split_vcf_cmd, shell=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_vcfs = [f\"{os.environ['INPUT_DIR']}/{fname}\" for fname in file_names if \".tbi\" not in fname]\n",
    "\n",
    "print(\"all files...\")\n",
    "for input_vcf in raw_vcfs:\n",
    "    print(input_vcf)\n",
    "    assert os.path.exists(input_vcf), \"file doesn't exist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define file names\n",
    "patient_stem_by_chr = [f\"{os.environ['SPLIT_DIR']}/{patient}.chr{c}\" for c in chrs_of_interest]\n",
    "patient_unfiltered_vcfs = [f\"{stem}.recode.vcf\" for stem in patient_stem_by_chr]\n",
    "patient_vcfs = [f\"{stem}.filtered.vcf\" for stem in patient_stem_by_chr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get patient-level for single chr\n",
    "for i, c in enumerate(chrs_of_interest):\n",
    "    raw_vcf = raw_vcfs[i]\n",
    "    unfiltered_vcf = patient_unfiltered_vcfs[i]\n",
    "    filtered_vcf = patient_vcfs[i]\n",
    "\n",
    "    # split vcf by patient\n",
    "    if os.path.exists(unfiltered_vcf):\n",
    "        print(f\"already split file: {unfiltered_vcf}\")\n",
    "    else:\n",
    "        # why is this taking 15 minutes\n",
    "        split_vcf_cmd = f\"bcftools view -s {patient} {raw_vcf}\" \\\n",
    "                        f\" > {unfiltered_vcf}\"\n",
    "\n",
    "        output = subprocess.run(split_vcf_cmd, shell=True, check=True)\n",
    "        \n",
    "    # filter to only relevant genotypes\n",
    "    if os.path.exists(filtered_vcf):\n",
    "        print(f\"already filtered file: {filtered_vcf}\")\n",
    "    else:\n",
    "        filter_genotypes_cmd = f'bcftools view -e \\'GT=\"0|0\"\\'' \\\n",
    "                        f\"{unfiltered_vcf} > {filtered_vcf}\"\n",
    "        print(filter_genotypes_cmd)\n",
    "        subprocess.run(filter_genotypes_cmd, shell=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OPTIONAL\n",
    "# # filter to first num_lines\n",
    "\n",
    "# num_lines = 1000\n",
    "# head_vcf = f\"{patient_path_stem}.{num_lines}.vcf\"\n",
    "\n",
    "# head_cmd = f\"cat {filtered_patient_vcf_path} | head -n {num_lines} > {head_vcf}\"\n",
    "# output = subprocess.run(head_cmd, shell=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !find ~ -name *1000.vcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking my work, make sure we've filtered without losing data\n",
    "\n",
    "for input_vcf, output_vcf, c in zip(patient_unfiltered_vcfs, patient_vcfs, chrs_of_interest):\n",
    "    print(c,\"~\")\n",
    "    rows = 0\n",
    "    \n",
    "    search_terms = [\"0|1\", \"1|0\", \"1|1\", \"0|0\", \"^#\"]\n",
    "    for term in search_terms:\n",
    "        numRows = int(subprocess.run(f\"grep '{term}' {input_vcf} | wc -l\", \\\n",
    "                      shell=True, check=True, stdout=subprocess.PIPE, text=True).stdout)\n",
    "        print(numRows)\n",
    "        rows += numRows\n",
    "        \n",
    "    expected_rows = int(subprocess.run(f\"grep '.' {input_vcf} | wc -l\", \\\n",
    "               shell=True, check=True, stdout=subprocess.PIPE, text=True).stdout)\n",
    "    assert rows == expected_rows, f\"chr{c}: rows {rows} not same as expected {expected_rows}\"\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get VRS Allele Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_vcf(input_vcf, output_vcf, output_pkl, seqrepo_root_dir, require_validation=True, rle_seq_limit=50):\n",
    "    '''param stem: path of input vcf file'''\n",
    "    vcf_annotator = VCFAnnotator(seqrepo_root_dir=seqrepo_root_dir)\n",
    "    vcf_annotator.tlr.rle_seq_limit = rle_seq_limit\n",
    "    vcf_annotator.annotate(vcf_in=input_vcf, vcf_out=output_vcf, \\\n",
    "        vrs_pickle_out=output_pkl, require_validation=require_validation)\n",
    "    \n",
    "    return output_vcf, output_pkl\n",
    "\n",
    "# create annotated vcf test file \n",
    "for curr_input_vcf in patient_vcfs:\n",
    "    stem = curr_input_vcf.replace('.vcf', '')\n",
    "    output_vcf = f\"{stem}.vcf.gz\"\n",
    "    output_pkl = f\"{stem}-vrs-objects.pkl\"\n",
    "\n",
    "    # write to file if doesn't exist\n",
    "    if os.path.exists(output_pkl):\n",
    "        print(\"output files already exists:\")\n",
    "#         print(f\" -- {output_vcf}\")\n",
    "        print(f\" -- {output_pkl}\")\n",
    "        continue\n",
    "\n",
    "    print(\"writing to...\")\n",
    "    print(output_vcf)\n",
    "    print(output_pkl)\n",
    "\n",
    "    t = time()\n",
    "    annotate_vcf(curr_input_vcf, output_vcf, output_pkl, SEQREPO_DIR)\n",
    "    elapsed_time = time()-t\n",
    "    print(f\"annotation: {(elapsed_time):.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query for MetaKB Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in metakb data locally\n",
    "\n",
    "METAKB_DIR = f\"{os.environ['HOME']}/metakb\"\n",
    "Path(METAKB_DIR).mkdir(exist_ok=True)\n",
    "\n",
    "json_files = [\"civic_cdm_20240103.json\", \"moa_cdm_20240103.json\"]\n",
    "json_paths = [f\"{METAKB_DIR}/{json_file}\" for json_file in json_files]\n",
    "\n",
    "for json_file, json_path in zip(json_files, json_paths):\n",
    "    if os.path.exists(json_path):\n",
    "        print(f\"{json_file} already exists...\")\n",
    "        continue\n",
    "    \n",
    "    url = f\"https://vicc-metakb.s3.us-east-2.amazonaws.com/cdm/20240103/{json_file}.zip\"\n",
    "    zip_path = f\"{json_path}.zip\"\n",
    "    zip_path_wrapped = Path(zip_path)\n",
    "    \n",
    "    download_s3(url, zip_path)\n",
    "    \n",
    "    print(zip_path)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(f\"{METAKB_DIR}\")\n",
    "    os.remove(zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create metakb cache of vrs ids\n",
    "\n",
    "metakb_cache = set()\n",
    "\n",
    "def metakb_ids_from_json(path):\n",
    "    allele_str = \"ga4gh:VA\"\n",
    "    ids = []\n",
    "    \n",
    "    with open(path) as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # studies\n",
    "    for study in data['studies']:\n",
    "        variant = study['variant']\n",
    "        \n",
    "        if 'members' in variant:\n",
    "            for member in variant['members']:\n",
    "                if allele_str not in member['id']:\n",
    "                    print(member['id'])\n",
    "            ids.extend([member['id'] for member in variant['members']])\n",
    "            \n",
    "        if 'definingContext' in variant:\n",
    "            if allele_str not in variant['definingContext']['id']:\n",
    "                print(variant['definingContext']['id'])\n",
    "            ids.append(variant['definingContext']['id'])\n",
    "    \n",
    "    # molecular_profiles\n",
    "    for profile in data['molecular_profiles']:\n",
    "        if 'members' in profile:\n",
    "            for member in profile['members']:\n",
    "                if allele_str not in member['id']:\n",
    "                    print(member['id'])\n",
    "            ids.extend([member['id'] for member in variant['members']])\n",
    "            \n",
    "        if 'definingContext' in profile:\n",
    "            if allele_str not in profile['definingContext']['id']:\n",
    "                print(profile['definingContext']['id'])\n",
    "            ids.append(profile['definingContext']['id'])\n",
    "    \n",
    "    # variations\n",
    "    ids.extend([variation['id'] for variation in data['variations'] \\\n",
    "                if allele_str in variation['id']])\n",
    "    \n",
    "    for variation in data['variations']:\n",
    "        if 'definingContext' in variation:\n",
    "            new_id = variation['definingContext']['id']\n",
    "            if allele_str in new_id:\n",
    "                ids.append(new_id)\n",
    "        \n",
    "    return ids\n",
    "\n",
    "# add from each CDM (json)\n",
    "t = time()\n",
    "for path in json_paths:\n",
    "    ids = metakb_ids_from_json(path)\n",
    "    print(f\"{path}: {len(ids)} ids\")\n",
    "    metakb_cache.update(ids)\n",
    "    \n",
    "print(f\"cache size: {len(metakb_cache)} ids\")\n",
    "print(f\"examples: {list(metakb_cache)[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for metakb matches\n",
    "\n",
    "id_start = 0\n",
    "id_end = None\n",
    "\n",
    "for i, curr_input_vcf in enumerate(patient_vcfs):\n",
    "    print(f\"trying chr{chrs_of_interest[i]}...\")\n",
    "    \n",
    "    # initial file names\n",
    "    stem = curr_input_vcf.replace('.vcf', '')\n",
    "    output_vcf = f\"{stem}.vcf.gz\"\n",
    "    output_pkl = f\"{stem}-vrs-objects.pkl\"\n",
    "    metakb_output_pkl = f\"{stem}-hits.pkl\"\n",
    "\n",
    "    # get ratio of alleles to variants\n",
    "    t = time()\n",
    "    allele_dicts = unpickle(output_pkl) # TODO: returns generator\n",
    "    num_alleles = sum(1 for _ in unpickle(output_pkl))\n",
    "    num_variants = get_num_variants(curr_input_vcf)\n",
    "    if id_end is None: id_end = num_alleles\n",
    "        \n",
    "    print(f\"num_alleles (ref and alt) to num_variants:\", end=\" \")\n",
    "    print_percent(num_alleles, num_variants)\n",
    "    print(f\"get percentage: {(time()-t):.2f} s\")\n",
    "\n",
    "    print(f\"writing to {metakb_output_pkl}\")\n",
    "\n",
    "    # convert alleles to vrs ids\n",
    "    t = time()\n",
    "    vrs_ids = [ga4gh_identify(models.Allele(**ast.literal_eval(allele_dict))) \\\n",
    "                for i, (_, allele_dict) in enumerate(allele_dicts) \\\n",
    "                if i >= id_start and i < id_end]\n",
    "    print(f\"{id_end-id_start} ids: {(time()-t):.2f} s\")\n",
    "\n",
    "    # ping metakb if cache hit\n",
    "    print(\"pinging metakb...\")\n",
    "    t = time()\n",
    "    id_hits = [vrs_id for vrs_id in vrs_ids if vrs_id in metakb_cache]\n",
    "    #     hits = [metakb(vrs_id) for vrs_id in id_hits]\n",
    "    print(f\"metakb: {(time()-t):.2f} s\")\n",
    "\n",
    "    with open(metakb_output_pkl, 'wb') as file:\n",
    "        pickle.dump(id_hits, file)\n",
    "\n",
    "    print(\"id hits\", id_hits)\n",
    "    print(\"\\nhits to ids queried...\")\n",
    "    # total = num_ids_limit if num_ids_limit else len(vrs_ids)\n",
    "    total = id_end - id_start\n",
    "    print_percent(len(id_hits), total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get evidence about the id and save all to pickle\n",
    "# TODO: replace it with metakb results\n",
    "\n",
    "def find_keys_from_ids(data, target=\"ga4gh:VA\", prefix=\"\", result=None):\n",
    "    if result is None:\n",
    "        result = set()\n",
    "        \n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            new_prefix = f\"{prefix}.{key}\" if prefix else key\n",
    "            if isinstance(value, (dict, list)):\n",
    "                find_keys_from_ids(value, target, new_prefix, result)\n",
    "            elif isinstance(value, str) and target in value:\n",
    "                result.add(new_prefix)\n",
    "    elif isinstance(data, list):\n",
    "        for index, value in enumerate(data):\n",
    "            new_prefix = f\"{prefix}.{index}\"\n",
    "            if isinstance(value, (dict, list)):\n",
    "                find_keys_from_ids(value, target, new_prefix, result)\n",
    "            elif isinstance(value, str) and target in value:\n",
    "                result.add(new_prefix)\n",
    "    return result\n",
    "\n",
    "# get evidence key drill down for each ting\n",
    "\n",
    "metakb_dict = {}\n",
    "for vrs_id in id_hits:\n",
    "    all_evidence_keys = []\n",
    "    for json_path in json_paths:\n",
    "        with open(json_path) as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        print(json_path)\n",
    "        associated_keys = find_keys_from_ids(data, vrs_id)\n",
    "        all_evidence_keys.extend(associated_keys)\n",
    "        print(associated_keys, \"\\n\")\n",
    "    \n",
    "    metakb_dict[vrs_id] = all_evidence_keys\n",
    "        \n",
    "with open(metakb_output_pkl, 'wb') as file:\n",
    "    pickle.dump(metakb_dict, file)\n",
    "\n",
    "with open(metakb_output_pkl, 'rb') as file:\n",
    "    print(\"final metakb_dict...\")\n",
    "    pprint(pickle.load(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example for using keys with chr\n",
    "\n",
    "with open(json_paths[0]) as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# pretty_print_json_tree(data[\"studies\"][45], print_values=True)\n",
    "for num in [38, 45, 71]:\n",
    "    study = data[\"studies\"]\n",
    "    print(f\"{study[num]['id']}: {study[num]['description']} \\n\")\n",
    "    \n",
    "print(f'Molecular profile aliases: {data[\"molecular_profiles\"][20][\"aliases\"]}')\n",
    "print(f'Molecular profile aliases: {data[\"molecular_profiles\"][31][\"aliases\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initial file names\n",
    "# curr_input_vcf = filtered_patient_vcf_path\n",
    "# stem = curr_input_vcf.replace('.vcf', '')\n",
    "# output_vcf = f\"{stem}.vcf.gz\"\n",
    "# output_pkl = f\"{stem}-vrs-objects.pkl\"\n",
    "\n",
    "# # get total num_variants\n",
    "# t = time()\n",
    "# allele_dicts = unpickle(output_pkl)\n",
    "# num_variants = get_num_variants(curr_input_vcf)\n",
    "# print(f'num_vrs_objects to num_variants: {len(allele_dicts)}/{num_variants}={100*(len(allele_dicts)/num_variants):.2f}%')\n",
    "# print(f\"get total: {(time()-t):.2f} s\")\n",
    "\n",
    "\n",
    "\n",
    "# # set number of ids to process\n",
    "# id_start = 20_000\n",
    "# id_end = 100_000 # ids to process\n",
    "# progress_interval = 10_000\n",
    "# metakb_output_pkl = f\"{patient_path_stem}-{id_start}-to-{id_end}-hits.pkl\"\n",
    "\n",
    "# print(f\"writing to {metakb_output_pkl}\")\n",
    "\n",
    "# # convert alleles to vrs ids\n",
    "# t = time()\n",
    "# vrs_ids = [ga4gh_identify(models.Allele(**allele_dict)) \\\n",
    "#             for i, (_, allele_dict) in enumerate(allele_dicts.items()) \\\n",
    "#             if i >= id_start and i < id_end]\n",
    "# print(f\"{id_end-id_start} ids: {(time()-t):.2f} s\")\n",
    "\n",
    "# # number of workers\n",
    "# worker_count = 4 * os.cpu_count()\n",
    "\n",
    "# # ping metakb\n",
    "# print(\"pinging metakb...\")\n",
    "# t = time()\n",
    "# hits = parallelize(metakb, vrs_ids, worker_count=worker_count, \\\n",
    "#     progress_interval=progress_interval)\n",
    "# print(f\"metakb: {(time()-t):.2f} s\")\n",
    "\n",
    "# with open(metakb_output_pkl, 'wb') as file:\n",
    "#     pickle.dump(hits, file)\n",
    "\n",
    "# print(\"\\nhits to ids queried...\")\n",
    "# # total = num_ids_limit if num_ids_limit else len(vrs_ids)\n",
    "# total = id_end - id_start\n",
    "# print_percent(len(hits), total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc Ways to look at CDM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_json_tree(data, max_depth=None, current_depth=0, indent=0, print_values=False):\n",
    "    if max_depth is not None and current_depth > max_depth:\n",
    "        return\n",
    "\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            print(\"  \" * indent + str(key))\n",
    "            pretty_print_json_tree(value, max_depth, current_depth + 1, indent + 1, print_values)\n",
    "    elif isinstance(data, list):\n",
    "        if data:  # Check if the list is not empty\n",
    "            pretty_print_json_tree(data[0], max_depth, current_depth, indent, print_values)\n",
    "    elif print_values:\n",
    "            print(\"  \" * indent + str(data))\n",
    "\n",
    "\n",
    "with open(json_paths[0]) as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "pretty_print_json_tree(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_keys_with_partial_match(data, target, prefix=\"\", result=None):\n",
    "    if result is None:\n",
    "        result = set()\n",
    "        \n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            new_prefix = f\"{prefix}.{key}\" if prefix else key\n",
    "            if isinstance(value, (dict, list)):\n",
    "                find_keys_with_partial_match(value, target, new_prefix, result)\n",
    "            elif isinstance(value, str) and target in value:\n",
    "                result.add(new_prefix)\n",
    "    elif isinstance(data, list):\n",
    "        for value in data:\n",
    "            if isinstance(value, (dict, list)):\n",
    "                find_keys_with_partial_match(value, target, prefix, result)\n",
    "            elif isinstance(value, str) and target in value:\n",
    "                result.add(prefix)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "target_value = \"ga4gh:VA\"\n",
    "\n",
    "for path in json_paths:\n",
    "    with open(path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "    keys_with_value = find_keys_with_partial_match(data, target_value)\n",
    "    print(path)\n",
    "    print(keys_with_value, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split before annotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! (seq 1 22; echo X; echo Y) | xargs -P 0 -I PATH $VCFTOOLS --recode --vcf \"/home/jupyter/vcf/1KGP_haplotype_caller_NA12878.chr10.hc.vcf\" --chr chrPATH --out $SPLIT_DIR/chrPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcf_path = drs_vcfs[0]\n",
    "\n",
    "! rm -r $SPLIT_DIR\n",
    "split_vcf_cmd = f\"(seq 1 22; echo X; echo Y) | \\\n",
    "               xargs -P 0 -I PATH $VCFTOOLS --recode --gzvcf {vcf_path} \\\n",
    "               --chr chrPATH --out $SPLIT_DIR/chrPATH\"\n",
    "\n",
    "output = subprocess.run(split_vcf_cmd, shell=True, check=True)\n",
    "# output = subprocess.run(split_vcf_cmd, shell=True, check=True, \\\n",
    "#                         capture_output=True, text=True) \n",
    "\n",
    "# no chr prefix\n",
    "# ! (seq 1 22; echo X; echo Y) | xargs -P 0 -I PATH ~/vcftools-vcftools-d511f46/src/cpp/vcftools --recode --vcf $VCF_PATH --chr PATH --out ~/split/chrPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: parse logs to get outputs on how many were filtered out\n",
    "# get total num_variants\n",
    "\n",
    "def get_num_variants(path):\n",
    "    vcf_reader = pysam.VariantFile(open(path, 'r'))\n",
    "    return sum(1 for record in vcf_reader)\n",
    "\n",
    "split_vcf_paths = glob.glob(f\"{os.environ.get('SPLIT_DIR')}/*.recode.vcf\")\n",
    "     \n",
    "input_num_variants = get_num_variants(vcf_path[:-3])\n",
    "split_num_variants = sum(get_num_variants(path) for path in split_vcf_paths)\n",
    "\n",
    "print(f\"{split_num_variants}/{input_num_variants} = \", \\\n",
    "      f\"{100*split_num_variants/input_num_variants:.2f}% kept\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -l $SPLIT_DIR/*.recode.vcf | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate each of them\n",
    "# TODO: fix the outputs coming from this\n",
    "\n",
    "! (ls -1 $SPLIT_DIR/*.recode.vcf | \\\n",
    "   xargs -P 0 -I PATH python3 -m ga4gh.vrs.extras.vcf_annotation \\\n",
    "   --vcf_in PATH --vcf_out PATH.vcf.gz --vrs_pickle_out PATH.pkl \\\n",
    "   --seqrepo_root_dir $SEQREPO_ROOT/latest \\\n",
    "   2> $SPLIT_DIR/chrPATH_log.txt)\n",
    "\n",
    "# # GREGoR\n",
    "# !python3 -m ga4gh.vrs.extras.vcf_annotation --vcf_in 1369747.merged.matefixed.sorted.markeddups.recal.g.vcf  --vcf_out 1369747.merged.matefixed.sorted.markeddups.recal.g.vcf.output.vcf.gz --vrs_pickle_out 1369747.merged.matefixed.sorted.markeddups.recal.g.vcf.vrs_objects.pkl  --seqrepo_root_dir ~/seqrepo/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l $SPLIT_DIR/*.vcf.vcf.gz | wc -l\n",
    "!ls -l $SPLIT_DIR/*.vcf.pkl | wc -l\n",
    "\n",
    "# assert (!ls -l ~/split/*.vcf.vcf.gz | wc -l) == 24, \"incorrect number of output vcf.gz files created\"\n",
    "# assert (!ls -l ~/split/*.vcf.pkl | wc -l) == 24, \"incorrect number of outputted pickle files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the files\n",
    "!ls -1 $SPLIT_DIR/*.vcf.vcf.gz | xargs $PERL5LIB/vcf-concat > $OUTPUT/merged_output.vcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: remove the pair of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random python annotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"ga4gh.vrs.extras.vcf_annotation\")\n",
    "logger.setLevel(level=logging.INFO)\n",
    "\n",
    "# create annotated vcf test file \n",
    "def annotate_vcf(path):\n",
    "    '''param stem: path of input vcf file'''\n",
    "    stem = path.replace(\".vcf\", \"\")\n",
    "    \n",
    "    input_vcf = path\n",
    "    output_vcf = f\"{stem}.output.vcf.gz\"\n",
    "    output_pkl = f\"{stem}-vrs-objects.pkl\"\n",
    "\n",
    "    \n",
    "    vcf_annotator = VCFAnnotator(seqrepo_root_dir=\"/home/jupyter/seqrepo/latest\")\n",
    "    vcf_annotator.annotate(vcf_in=input_vcf, vcf_out=output_vcf, vrs_pickle_out=output_pkl)\n",
    "    # vcf_annotator.annotate(vcf_in=input_vcf, vrs_pickle_out=output_pkl)\n",
    "    \n",
    "# annotate_vcf(\"/home/jupyter/split\", \"chr1.recode\")\n",
    "successes = set()\n",
    "for vcf_path in drs_vcfs:\n",
    "    try:\n",
    "        print(\"trying...\", vcf_path)\n",
    "        annotate_vcf(vcf_path)\n",
    "        print(\"worked \\n\")\n",
    "        successes.add(vcf_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"unsucessful, see logs above \\n\")\n",
    "\n",
    "print(f\"total successes: {len(successes)}/{len(drs_vcfs)} \\nList...\")\n",
    "for vcf_path in drs_vcfs:\n",
    "    print(f\"{vcf_path}: {'✓' if vcf_path in successes else 'x'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate w vrs id asking for output vcf\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"ga4gh.vrs.extras.vcf_annotation\")\n",
    "logger.setLevel(level=logging.INFO)\n",
    "\n",
    "# create annotated vcf test file \n",
    "def annotate_vcf(path):\n",
    "    '''param stem: path of input vcf file'''\n",
    "    stem = path.replace(\".vcf\", \"\")\n",
    "    \n",
    "    input_vcf = path\n",
    "    output_vcf = f\"{stem}.output.vcf.gz\"\n",
    "    output_pkl = f\"{stem}-vrs-objects.pkl\"\n",
    "\n",
    "    \n",
    "    vcf_annotator = VCFAnnotator(seqrepo_root_dir=\"/home/jupyter/seqrepo/latest\")\n",
    "    vcf_annotator.annotate(vcf_in=input_vcf, vcf_out=output_vcf, vrs_pickle_out=output_pkl)\n",
    "    # vcf_annotator.annotate(vcf_in=input_vcf, vrs_pickle_out=output_pkl)\n",
    "    \n",
    "# annotate_vcf(\"/home/jupyter/split\", \"chr1.recode\")\n",
    "successes = set()\n",
    "for vcf_path in drs_vcfs:\n",
    "    try:\n",
    "        print(\"trying...\", vcf_path)\n",
    "        annotate_vcf(vcf_path)\n",
    "        print(\"worked \\n\")\n",
    "        successes.add(vcf_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"unsucessful, see logs above \\n\")\n",
    "\n",
    "print(f\"total successes: {len(successes)}/{len(drs_vcfs)} \\nList...\")\n",
    "for vcf_path in drs_vcfs:\n",
    "    print(f\"{vcf_path}: {'✓' if vcf_path in successes else 'x'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vcf_path in drs_vcfs:\n",
    "    if \"HG02080vCHM13_20200921\" in vcf_path:\n",
    "        print(vcf_path)\n",
    "    else:\n",
    "        continue\n",
    "#     if \"chm13_hifi_HG007\" in vcf_path:\n",
    "#         print(\"trying...\", vcf_path)\n",
    "#         annotate_vcf(vcf_path)\n",
    "#         print(\"worked \\n\")\n",
    "    try:\n",
    "        print(\"trying...\", vcf_path)\n",
    "        annotate_vcf(vcf_path)\n",
    "        print(\"worked \\n\")\n",
    "        successes.add(vcf_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"unsucessful, see logs above \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate w vrs id only pickle outputted\n",
    "\n",
    "logger = logging.getLogger(\"ga4gh.vrs.extras.vcf_annotation\")\n",
    "# logger.setLevel(level=logging.ERROR)\n",
    "logger.disabled = True\n",
    "\n",
    "# create annotated vcf test file \n",
    "def annotate_vcf_pkl_only(path):\n",
    "    '''param stem: path of input vcf file'''\n",
    "    stem = path.replace(\".vcf\", \"\")\n",
    "    \n",
    "    input_vcf = path\n",
    "    output_vcf = f\"{stem}.output.vcf.gz\"\n",
    "    output_pkl = f\"{stem}-vrs-objects.pkl\"\n",
    "\n",
    "    \n",
    "    vcf_annotator = VCFAnnotator(seqrepo_root_dir=\"/home/jupyter/seqrepo/latest\")\n",
    "    vcf_annotator.annotate(vcf_in=input_vcf, vrs_pickle_out=output_pkl)\n",
    "    \n",
    "successes = set()\n",
    "for i, vcf_path in enumerate(drs_vcfs):\n",
    "    print(\"starting... \\n\")\n",
    "    # annotate to output pkl\n",
    "    try:\n",
    "        print(\"trying...\", vcf_path)\n",
    "        annotate_vcf_pkl_only(vcf_path)\n",
    "        print(\"worked \\n\")\n",
    "        successes.add(vcf_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"unsucessful, see logs above \\n\")\n",
    "    \n",
    "    # get pickle totals\n",
    "    try:\n",
    "        with open(output_pkl, 'rb') as f:\n",
    "            vrs_objects = pickle.load(f)\n",
    "\n",
    "        # get total num_variants\n",
    "        vcf_reader = vcf.Reader(open(vcf_path, 'r'))\n",
    "        num_variants = sum(1 for record in vcf_reader)\n",
    "\n",
    "        # view details\n",
    "        print(f'num_vrs_objects to num_varaints: {len(vrs_objects)}/{num_variants}={(len(vrs_objects)/num_variants):.2f}%')\n",
    "    except:\n",
    "        print(\"unable to get pickle totals, file may not exist\")\n",
    "    print()\n",
    "\n",
    "print(f\"total successes: {len(successes)}/{len(drs_vcfs)} \\nList...\")\n",
    "for vcf_path in drs_vcfs:\n",
    "    print(f\"{vcf_path}: {'✓' if vcf_path in successes else 'x'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vcf\n",
    "\n",
    "\n",
    "# for input_vcf_file in [\"/home/jupyter/vcf/long_read_sv_jasmine_Trios_IndividualCallsets_CHM13_HG005_Trio_HG006vCHM13_20200921_mm2_PBCCS_sniffles.s2l20.refined.nSVtypes.ism.vcf\"]:\n",
    "for input_vcf_file in [\"/home/jupyter/vcf/long_read_minimap2_alignments_HG02080vCHM13_20200921_mm2_ONT_sniffles.s2l20.refined.nSVtypes.ism.vcf\"]:\n",
    "    output_vcf_file = \"/home/jupyter/vcf/long_read.test.vcf\"\n",
    "\n",
    "    vcf_reader = vcf.Reader(open(input_vcf_file, 'r'))\n",
    "    vcf_writer = vcf.Writer(open(output_vcf_file, 'w'), vcf_reader)\n",
    "\n",
    "    for record in vcf_reader:\n",
    "        record.INFO['VRS_ALLELE_ID'] = 'ga4gh:VA.xksahgfowdfdwofd,ga4gh:VA.xksahgfowdfdwofd'\n",
    "        vcf_writer.write_record(record)\n",
    "\n",
    "vcf_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show loaded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# import pickle\n",
    "# import ast\n",
    "# import requests\n",
    "# import datetime\n",
    "\n",
    "# # log progress\n",
    "# progress_interval = 50000\n",
    "\n",
    "# # load pickled dict\n",
    "# with open(output_pkl, 'rb') as f:\n",
    "#     print(datetime.datetime.now().isoformat(), 'opened pickle')\n",
    "#     vrs_objects = pickle.load(f)\n",
    "#     c = 0\n",
    "#     for k, v in vrs_objects.items():\n",
    "#         vrs_objects[k] = ast.literal_eval(v)\n",
    "#         c += 1\n",
    "#         if c % progress_interval == 0:\n",
    "#             print(datetime.datetime.now().isoformat(), c)\n",
    "\n",
    "# # view details        \n",
    "# print('number of vrs objects', len(vrs_objects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_paths = !ls -1 ~/split/*.vcf.pkl\n",
    "pickle_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get percent of loaded variants\n",
    "\n",
    "# load pickled dict\n",
    "# for vcf_path in drs_vcfs:\n",
    "\n",
    "def unpickle_generator(file_name):\n",
    "    \"\"\"Unpickle vrs objects, yields (key,vrs_object)\"\"\"\n",
    "    with open(file_name, 'rb') as f:\n",
    "        vrs_objects = pickle.load(f)\n",
    "        for k, v in vrs_objects.items():\n",
    "            yield k, ast.literal_eval(v)\n",
    "            \n",
    "def unpickle(file_name):\n",
    "    \"\"\"Unpickle vrs objects to single dict\"\"\"\n",
    "    with open(file_name, 'rb') as f:\n",
    "        vrs_objects = pickle.load(f)\n",
    "        for k, v in vrs_objects.items():\n",
    "            vrs_objects[k] = ast.literal_eval(v)\n",
    "    \n",
    "    return vrs_objects\n",
    "\n",
    "vrs_dicts = []\n",
    "\n",
    "total_num_vrs_objs = 0\n",
    "\n",
    "for path in pickle_paths:\n",
    "    vrs_dict = unpickle(path)\n",
    "    vrs_dicts.append(vrs_dict)\n",
    "\n",
    "    # get total num_variants\n",
    "    # TODO: reference the new merged file bc some might have been filtered out\n",
    "    vcf_reader = vcf.Reader(open(path[:-4], 'r'))\n",
    "    num_variants = sum(1 for record in vcf_reader)\n",
    "\n",
    "#     num_vrs_objs = sum((1 for _ in vrs_objects))\n",
    "    num_vrs_objs = len(vrs_dict)\n",
    "    total_num_vrs_objs += num_vrs_objs\n",
    "\n",
    "    # view details\n",
    "    \n",
    "    print(path.split(\"/\")[-1], end=\" \")\n",
    "    if num_variants == 0: \n",
    "        print(f\"no variants\") \n",
    "    else:\n",
    "        print(f'vrs_objects:variants = {num_vrs_objs}/{num_variants} = {(50*num_vrs_objs/num_variants):.1f}%')\n",
    "\n",
    "total_variants = get_num_variants(vcf_path)\n",
    "        \n",
    "print(f\"Totals: {total_num_vrs_objs}/{total_variants}\", \\\n",
    "      f\"= {(50*total_num_vrs_objs/total_variants):.2f}%\")\n",
    "        \n",
    "# TODO on combining: have to think about this more bc large files will have to be held in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error reporting from logs\n",
    "num_val_errors = !(grep \"raise ValidationError(err_msg)\" $HOME/log.txt | wc -l)\n",
    "num_val_errors = int(num_val_errors[0])\n",
    "print(f\"validations errors = {num_val_errors}, ie {50*num_val_errors/total_variants:.1f}%\", \\\n",
    "      \" if 2:1 VRS ID to variant\")\n",
    "\n",
    "num_invalid_files = !(grep \"\\[E::vcf_format\\] Invalid BCF\" $HOME/log.txt | wc -l)\n",
    "num_invalid_files = int(num_invalid_files[0])\n",
    "print(f\"num invalid files: {num_invalid_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
