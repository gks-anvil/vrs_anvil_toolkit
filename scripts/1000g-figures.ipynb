{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Downstream Usage: 1000g Figures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get statistics from 1000g samples, including\n",
    "  - total samples\n",
    "  - % matches to each kb individually and metakb as a whole\n",
    "  - study ids and descriptions by variant hit in metakb\n",
    "\n",
    "(to be run in a tmp/ directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install --upgrade --no-cache-dir terra-notebook-utils\n",
    "%env REPO_DIR=/home/jupyter/vrs-python-testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# completes setup for Terra\n",
    "!cd $REPO_DIR && bash terra/setup.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates cohort allele frequency dict, make sure\n",
    "!python $REPO_DIR/scripts/1000g-processing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import pysam\n",
    "import seaborn as sns\n",
    "import subprocess\n",
    "import yaml\n",
    "\n",
    "from collections import defaultdict\n",
    "from google.cloud import storage\n",
    "from glob import glob\n",
    "from glom import glom\n",
    "from firecloud import api as fapi\n",
    "from re import match\n",
    "from vrs_anvil import query_metakb\n",
    "from vrs_anvil.annotator import MATCHES, TOTAL, VRS_OBJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants to load and save files\n",
    "figure_dir = \"figures\"\n",
    "TIMESTAMP = \"20240329_013953\"\n",
    "BASE_DIR = \"/home/jupyter/vrs-python-testing/tmp\"\n",
    "\n",
    "# seaborn styling\n",
    "sns.set_theme()\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read Cohort Allele Frequency (CAF) objects to file\n",
    "caf_dir = os.path.expanduser(f\"{BASE_DIR}/state\")\n",
    "os.makedirs(caf_dir, exist_ok=True)\n",
    "file_name = f'caf_objects_{TIMESTAMP}.json'\n",
    "\n",
    "with open(f'{caf_dir}/{file_name}', 'r') as file:\n",
    "    caf_dicts = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example formatting of dictionary\n",
    "print(caf_dicts[0][\"id\"])\n",
    "print(caf_dicts[0][\"ancillaryResults\"][\"sample_dict\"]['HG00382'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted list of matches to local metakb\n",
    "sorted_caf_dicts = sorted(caf_dicts, \\\n",
    "                          key=lambda d: d['ancillaryResults']['patient_matches'], \\\n",
    "                           reverse=True)\n",
    "\n",
    "for caf_dict in sorted_caf_dicts:\n",
    "    print(caf_dict[\"id\"], \":\", caf_dict['ancillaryResults']['patient_matches'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to merge each sample dict from each caf into one dict \n",
    "def merge_sample_dict(sample_evidence_dict, allele_id, sample_dict):\n",
    "    '''update sample evidence for a given allele'''\n",
    "    \n",
    "    if allele_id in sample_evidence_dict:\n",
    "        print(allele_id, \"exists, skipping\")\n",
    "        return\n",
    "    \n",
    "    for sample, sample_info in sample_dict.items():\n",
    "        study_ids = sample_info[\"study_ids\"]\n",
    "        variant_types = sample_info[\"variant_types\"]\n",
    "        \n",
    "        if sample not in sample_evidence_dict:\n",
    "            sample_evidence_dict[sample] = {}\n",
    "\n",
    "        if \"study_ids\" in sample_evidence_dict[sample]:\n",
    "            sample_evidence_dict[sample][\"study_ids\"].extend(study_ids)\n",
    "            sample_evidence_dict[sample][\"variant_types\"].extend(variant_types)\n",
    "        else:\n",
    "            sample_evidence_dict[sample][\"study_ids\"] = list(study_ids)\n",
    "            sample_evidence_dict[sample][\"variant_types\"] = list(variant_types)\n",
    "\n",
    "# create evidence dict for all matches\n",
    "sample_evidence_dict = defaultdict(list)\n",
    "\n",
    "for caf_dict in sorted_caf_dicts:\n",
    "    merge_sample_dict(sample_evidence_dict, caf_dict[\"id\"], caf_dict[\"ancillaryResults\"][\"sample_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot vrs_ids sorted by number of patient matches\n",
    "num_patients_list = [d['ancillaryResults']['patient_matches'] for d in sorted_caf_dicts]\n",
    "ids = [d['id'] for d in sorted_caf_dicts]\n",
    "print(len(ids))\n",
    "plt.bar(ids, num_patients_list)\n",
    "plt.xticks(rotation=90, fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example therapeutic evidence\n",
    "\n",
    "for d in sorted_caf_dicts[:2]:\n",
    "    print(f'### {d[\"id\"]} ###')\n",
    "    for study in d[\"ancillaryResults\"][\"metakb_dict\"][\"studies\"]:\n",
    "        for key in [\"id\", \"type\", \"direction\", \"predicate\", \"therapeutic\", \"tumorType.label\", \"strength.label\"]:\n",
    "            v = glom(study, key)\n",
    "            if key == \"therapeutic\":\n",
    "                if \"substitutes\" in v:\n",
    "                    print([a[\"label\"] for a in v[\"substitutes\"]])\n",
    "                elif \"components\" in v:\n",
    "                    print([a[\"label\"] for a in v[\"components\"]])\n",
    "                    \n",
    "            else:\n",
    "                print(v)\n",
    "        \n",
    "        print(\"~~~~\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "knowledgebases = [\"MOAlmanac\", \"CIVIC\", \"All Knowledgebases\"]\n",
    "kb_keywords = [\"moa\", \"civic\", \"\"]  # \"\" represents match for any knowledgebase\n",
    "\n",
    "KB, PCT, VAR = range(3)\n",
    "cols = [\"knowledgebase\", \"percent\", \"variant_type\"]\n",
    "variant_types = [\"germline\", \"somatic\"]\n",
    "variant_types_set = set(variant_types)\n",
    "dtypes = [str, float, str]\n",
    "\n",
    "TOTAL = \"all\"\n",
    "num_samples = 3202\n",
    "variants = sorted(list(variant_types_set)) + [TOTAL]\n",
    "\n",
    "data = None\n",
    "\n",
    "# get percentage of patients with variant match by category\n",
    "for variant_type in variants:\n",
    "    for i, keyword in enumerate(kb_keywords):\n",
    "        num_matching_samples = 0\n",
    "\n",
    "        # increment if sample has matching variant and kb\n",
    "        for _, id_lists in sample_evidence_dict.items():\n",
    "            v_types = np.array(id_lists[\"variant_types\"])\n",
    "\n",
    "            if variant_type == TOTAL:\n",
    "                study_ids = np.array(id_lists[\"study_ids\"])\n",
    "            else:\n",
    "                study_ids = np.array(id_lists[\"study_ids\"])[v_types == variant_type]\n",
    "            if any(keyword in study_id for study_id in study_ids):\n",
    "                num_matching_samples += 1\n",
    "\n",
    "\n",
    "        percent = num_matching_samples * 100.0 / num_samples\n",
    "        if data is None:\n",
    "            data = np.array([knowledgebases[i], percent, variant_type])\n",
    "        else:\n",
    "            data = np.vstack([data, [knowledgebases[i], percent, variant_type]])\n",
    "        \n",
    "expected_num_rows = len(knowledgebases) * len(variants)\n",
    "assert len(data) == expected_num_rows, f\"expected {expected_num_rows} rows, got {len(data)}\"\n",
    "\n",
    "df_pct = pd.DataFrame(data, columns=cols).astype(dtype={col: dtype for col, dtype in zip(cols, dtypes)})\n",
    "print(\"Percent of Patients with a Single Variant Match\")\n",
    "df_pct[\"matching_samples\"] = (df_pct[\"percent\"]*num_samples/100).astype(int)\n",
    "\n",
    "df_display = df_pct.copy()\n",
    "df_display[\"percent\"] = df_display[\"percent\"].round(2)\n",
    "df_display.sort_values(by=[\"knowledgebase\", \"variant_type\"], ascending=[False, True])\n",
    "df_display[[\"knowledgebase\", \"variant_type\", \"percent\", \"matching_samples\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to add fractional labels to bars on plots\n",
    "def add_labels(num_bars, ax, vertical_nudge, num_samples, fractional=False, fontsize=8):\n",
    "    for i, bar in enumerate(ax.patches):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        if i > num_bars:\n",
    "            break\n",
    "\n",
    "        height = bar.get_height()\n",
    "        matching_samples = int(round(height * num_samples))\n",
    "        \n",
    "        \n",
    "        if not fractional:\n",
    "            label = f\"{matching_samples}\"\n",
    "        else:\n",
    "            label = f\"{matching_samples}/{num_samples}\"\n",
    "        x = bar.get_x() + bar.get_width() / 2\n",
    "        y = bar.get_y() + height + vertical_nudge\n",
    "        ax.text(x, y, label, ha=\"center\", va=\"center\", fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average number of variants for all samples\n",
    "num_variant_hits = len([1 for d in sorted_caf_dicts if \"metakb_dict\" in d[\"ancillaryResults\"]])\n",
    "jitter = [0.008, .008]\n",
    "\n",
    "\n",
    "NUM_VARIANTS = \"num_variants\"\n",
    "\n",
    "for i, variant_type in enumerate(variant_types):\n",
    "    num_variants_per_patient = [len( \\\n",
    "        [v for v in values[\"variant_types\"] if v == variant_type or variant_type == TOTAL]) \\\n",
    "        for values in sample_evidence_dict.values()]\n",
    "        \n",
    "    num_variants_per_patient.extend([0 for _ in range(num_samples - len(sample_evidence_dict))])\n",
    "    assert len(num_variants_per_patient) == num_samples, \\\n",
    "    f\"only {len(num_variants_per_patient)} samples, expected {num_samples}\"\n",
    "    \n",
    "    df = pd.DataFrame({NUM_VARIANTS: num_variants_per_patient})\n",
    "    df[\"percentage\"] = df[NUM_VARIANTS].value_counts(normalize=True) * 100\n",
    "    \n",
    "    plt.figure(dpi=200)\n",
    "    ax = sns.histplot(data=df, x=NUM_VARIANTS, stat=\"density\", discrete=True)\n",
    "    plt.xlabel(\"Number of Variants\")\n",
    "    plt.ylabel(\"Percentage of All Patients (%)\")\n",
    "    plt.title(f\"Number of {variant_type.capitalize()} Variants Associated with Each Patient\")\n",
    "    \n",
    "    add_labels(num_variant_hits, ax, jitter[i], num_samples)\n",
    "\n",
    "    plt.gca().yaxis.set_major_formatter(lambda x, _: f\"{(x*100):.0f}\")\n",
    "    plt.grid(False)\n",
    "    \n",
    "    x_lims = (0.5, max((df[NUM_VARIANTS])+1))\n",
    "    plt.xlim(0.5, max((df[NUM_VARIANTS])+1))\n",
    "    plt.xticks(range(1, max((df[NUM_VARIANTS])+1)))\n",
    "    plt.ylim(0, 0.5)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
