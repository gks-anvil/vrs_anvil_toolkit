{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VRS workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install seqrepo ga4gh.vrs[extras]==2.0.0a3 ga4gh.vrs\n",
    "%pip install --upgrade --no-cache-dir terra-notebook-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from firecloud import api as fapi\n",
    "from ga4gh.core import ga4gh_identify\n",
    "from ga4gh.vrs import models\n",
    "from ga4gh.vrs.extras.vcf_annotation import VCFAnnotator\n",
    "from pathlib import Path\n",
    "from terra_notebook_utils import drs\n",
    "from time import time \n",
    "\n",
    "import ast\n",
    "import datetime\n",
    "import glob\n",
    "import io\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os \n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pysam\n",
    "import requests\n",
    "import subprocess\n",
    "import vcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SEQREPO_ROOT=/home/jupyter/seqrepo\n",
      "env: VCFTOOLS_DIR=/home/jupyter/vcftools\n",
      "env: PERL5LIB=/home/jupyter/vcftools/src/perl/\n",
      "env: VCFTOOLS=/home/jupyter/vcftools/src/cpp/vcftools\n",
      "env: OUTPUT=/home/jupyter/output\n",
      "env: SPLIT_DIR=/home/jupyter/split\n",
      "env: INPUT_DIR=/home/jupyter/vcf\n",
      "mkdir: cannot create directory ‘/home/jupyter/vcf’: File exists\n",
      "mkdir: cannot create directory ‘/home/jupyter/split’: File exists\n",
      "mkdir: cannot create directory ‘/home/jupyter/output’: File exists\n"
     ]
    }
   ],
   "source": [
    "# store relevant variables\n",
    "\n",
    "%env SEQREPO_ROOT=/home/jupyter/seqrepo\n",
    "%env VCFTOOLS_DIR=/home/jupyter/vcftools\n",
    "%env PERL5LIB=/home/jupyter/vcftools/src/perl/\n",
    "%env VCFTOOLS=/home/jupyter/vcftools/src/cpp/vcftools\n",
    "%env OUTPUT=/home/jupyter/output\n",
    "%env SPLIT_DIR=/home/jupyter/split\n",
    "%env INPUT_DIR=/home/jupyter/vcf\n",
    "!mkdir $INPUT_DIR\n",
    "!mkdir $SPLIT_DIR\n",
    "!mkdir $OUTPUT\n",
    "\n",
    "SEQREPO_DIR = os.environ[\"SEQREPO_ROOT\"]+\"/latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seqrepo in /home/jupyter/.local/lib/python3.10/site-packages (from -r /home/jupyter/requirements.txt (line 1)) (0.0.0)\n",
      "Collecting ga4gh.vrs==2.0.0a2 (from ga4gh.vrs[extras]==2.0.0a2->-r /home/jupyter/requirements.txt (line 2))\n",
      "  Using cached ga4gh.vrs-2.0.0a2-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pyVCF (from -r /home/jupyter/requirements.txt (line 4))\n",
      "  Using cached PyVCF-0.6.8.tar.gz (34 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[1 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m error in PyVCF setup command: use_2to3 is invalid.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[?25hmkdir: cannot create directory ‘/home/jupyter/seqrepo’: File exists\n",
      "WARNING:biocommons.seqrepo.cli:2024-02-20: instance already exists; skipping\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  241k    0  241k    0     0   396k      0 --:--:-- --:--:-- --:--:--  396k\n",
      "curl: Saved to filename 'vcftools-vcftools-v0.1.16-20-gd511f46.tar.gz'\n",
      "vcftools-vcftools-d511f46/\n",
      "vcftools-vcftools-d511f46/.gitignore\n",
      "vcftools-vcftools-d511f46/.tarball-version\n",
      "vcftools-vcftools-d511f46/LICENSE\n",
      "vcftools-vcftools-d511f46/Makefile.am\n",
      "vcftools-vcftools-d511f46/README.md\n",
      "vcftools-vcftools-d511f46/autogen.sh\n",
      "vcftools-vcftools-d511f46/build-aux/\n",
      "vcftools-vcftools-d511f46/build-aux/git-version-gen\n",
      "vcftools-vcftools-d511f46/configure.ac\n",
      "vcftools-vcftools-d511f46/examples/\n",
      "vcftools-vcftools-d511f46/examples/annotate-test.vcf\n",
      "vcftools-vcftools-d511f46/examples/annotate.out\n",
      "vcftools-vcftools-d511f46/examples/annotate.txt\n",
      "vcftools-vcftools-d511f46/examples/annotate2.out\n",
      "vcftools-vcftools-d511f46/examples/annotate3.out\n",
      "vcftools-vcftools-d511f46/examples/cmp-test-a-3.3.vcf\n",
      "vcftools-vcftools-d511f46/examples/cmp-test-a.vcf\n",
      "vcftools-vcftools-d511f46/examples/cmp-test-b-3.3.vcf\n",
      "vcftools-vcftools-d511f46/examples/cmp-test-b.vcf\n",
      "vcftools-vcftools-d511f46/examples/cmp-test.out\n",
      "vcftools-vcftools-d511f46/examples/concat-a.vcf\n",
      "vcftools-vcftools-d511f46/examples/concat-b.vcf\n",
      "vcftools-vcftools-d511f46/examples/concat-c.vcf\n",
      "vcftools-vcftools-d511f46/examples/concat.out\n",
      "vcftools-vcftools-d511f46/examples/consensus.fa\n",
      "vcftools-vcftools-d511f46/examples/consensus.out\n",
      "vcftools-vcftools-d511f46/examples/consensus.out2\n",
      "vcftools-vcftools-d511f46/examples/consensus.vcf\n",
      "vcftools-vcftools-d511f46/examples/contrast.out\n",
      "vcftools-vcftools-d511f46/examples/contrast.vcf\n",
      "vcftools-vcftools-d511f46/examples/fill-an-ac.out\n",
      "vcftools-vcftools-d511f46/examples/filters.txt\n",
      "vcftools-vcftools-d511f46/examples/fix-ploidy.out\n",
      "vcftools-vcftools-d511f46/examples/fix-ploidy.samples\n",
      "vcftools-vcftools-d511f46/examples/fix-ploidy.txt\n",
      "vcftools-vcftools-d511f46/examples/fix-ploidy.vcf\n",
      "vcftools-vcftools-d511f46/examples/floats.vcf\n",
      "vcftools-vcftools-d511f46/examples/indel-stats.out\n",
      "vcftools-vcftools-d511f46/examples/indel-stats.tab\n",
      "vcftools-vcftools-d511f46/examples/indel-stats.vcf\n",
      "vcftools-vcftools-d511f46/examples/invalid-4.0.vcf\n",
      "vcftools-vcftools-d511f46/examples/isec-n2-test.vcf.out\n",
      "vcftools-vcftools-d511f46/examples/merge-test-a.vcf\n",
      "vcftools-vcftools-d511f46/examples/merge-test-b.vcf\n",
      "vcftools-vcftools-d511f46/examples/merge-test-c.vcf\n",
      "vcftools-vcftools-d511f46/examples/merge-test.vcf.out\n",
      "vcftools-vcftools-d511f46/examples/parse-test.vcf\n",
      "vcftools-vcftools-d511f46/examples/perl-api-1.pl\n",
      "vcftools-vcftools-d511f46/examples/query-test.out\n",
      "vcftools-vcftools-d511f46/examples/shuffle-test.vcf\n",
      "vcftools-vcftools-d511f46/examples/subset.SNPs.out\n",
      "vcftools-vcftools-d511f46/examples/subset.indels.out\n",
      "vcftools-vcftools-d511f46/examples/subset.vcf\n",
      "vcftools-vcftools-d511f46/examples/valid-3.3.vcf\n",
      "vcftools-vcftools-d511f46/examples/valid-4.0.vcf\n",
      "vcftools-vcftools-d511f46/examples/valid-4.0.vcf.stats\n",
      "vcftools-vcftools-d511f46/examples/valid-4.1.vcf\n",
      "vcftools-vcftools-d511f46/src/\n",
      "vcftools-vcftools-d511f46/src/Makefile.am\n",
      "vcftools-vcftools-d511f46/src/cpp/\n",
      "vcftools-vcftools-d511f46/src/cpp/Makefile.am\n",
      "vcftools-vcftools-d511f46/src/cpp/bcf_entry.cpp\n",
      "vcftools-vcftools-d511f46/src/cpp/bcf_entry.h\n",
      "vcftools-vcftools-d511f46/src/cpp/bcf_entry_setters.cpp\n",
      "vcftools-vcftools-d511f46/src/cpp/bcf_file.cpp\n",
      "vcftools-vcftools-d511f46/src/cpp/bcf_file.h\n",
      "vcftools-vcftools-d511f46/src/cpp/bgzf.c\n",
      "vcftools-vcftools-d511f46/src/cpp/bgzf.h\n",
      "vcftools-vcftools-d511f46/src/cpp/dgeev.cpp\n",
      "vcftools-vcftools-d511f46/src/cpp/dgeev.h\n",
      "vcftools-vcftools-d511f46/src/cpp/entry.cpp\n",
      "vcftools-vcftools-d511f46/src/cpp/entry.h\n",
      "vcftools-vcftools-d511f46/src/cpp/entry_filters.cpp\n",
      "vcftools-vcftools-d511f46/src/cpp/entry_getters.cpp\n",
      "vcftools-vcftools-d511f46/src/cpp/entry_setters.cpp\n",
      "vcftools-vcftools-d511f46/src/cpp/gamma.cpp\n",
      "vcftools-vcftools-d511f46/src/cpp/gamma.h\n",
      "vcftools-vcftools-d511f46/src/cpp/header.cpp\n",
      "vcftools-vcftools-d511f46/src/cpp/header.h\n",
      "vcftools-vcftools-d511f46/src/cpp/khash.h\n",
      "vcftools-vcftools-d511f46/src/cpp/knetfile.c\n",
      "vcftools-vcftools-d511f46/src/cpp/knetfile.h\n",
      "vcftools-vcftools-d511f46/src/cpp/output_log.cpp\n",
      "vcftools-vcftools-d511f46/src/cpp/output_log.h\n",
      "vcftools-vcftools-d511f46/src/cpp/parameters.cpp\n",
      "vcftools-vcftools-d511f46/src/cpp/parameters.h\n",
      "vcftools-vcftools-d511f46/src/cpp/variant_file.cpp\n",
      "vcftools-vcftools-d511f46/src/cpp/variant_file.h\n",
      "vcftools-vcftools-d511f46/src/cpp/variant_file_diff.cpp\n",
      "vcftools-vcftools-d511f46/src/cpp/variant_file_filters.cpp\n",
      "vcftools-vcftools-d511f46/src/cpp/variant_file_format_convert.cpp\n",
      "vcftools-vcftools-d511f46/src/cpp/variant_file_output.cpp\n",
      "vcftools-vcftools-d511f46/src/cpp/vcf_entry.cpp\n",
      "vcftools-vcftools-d511f46/src/cpp/vcf_entry.h\n",
      "vcftools-vcftools-d511f46/src/cpp/vcf_entry_setters.cpp\n",
      "vcftools-vcftools-d511f46/src/cpp/vcf_file.cpp\n",
      "vcftools-vcftools-d511f46/src/cpp/vcf_file.h\n",
      "vcftools-vcftools-d511f46/src/cpp/vcftools.1\n",
      "vcftools-vcftools-d511f46/src/cpp/vcftools.cpp\n",
      "vcftools-vcftools-d511f46/src/cpp/vcftools.h\n",
      "vcftools-vcftools-d511f46/src/perl/\n",
      "vcftools-vcftools-d511f46/src/perl/ChangeLog\n",
      "vcftools-vcftools-d511f46/src/perl/FaSlice.pm\n",
      "vcftools-vcftools-d511f46/src/perl/Makefile.am\n",
      "vcftools-vcftools-d511f46/src/perl/Vcf.pm\n",
      "vcftools-vcftools-d511f46/src/perl/VcfStats.pm\n",
      "vcftools-vcftools-d511f46/src/perl/fill-aa\n",
      "vcftools-vcftools-d511f46/src/perl/fill-an-ac\n",
      "vcftools-vcftools-d511f46/src/perl/fill-fs\n",
      "vcftools-vcftools-d511f46/src/perl/fill-ref-md5\n",
      "vcftools-vcftools-d511f46/src/perl/tab-to-vcf\n",
      "vcftools-vcftools-d511f46/src/perl/test.t\n",
      "vcftools-vcftools-d511f46/src/perl/vcf-annotate\n",
      "vcftools-vcftools-d511f46/src/perl/vcf-compare\n",
      "vcftools-vcftools-d511f46/src/perl/vcf-concat\n",
      "vcftools-vcftools-d511f46/src/perl/vcf-consensus\n",
      "vcftools-vcftools-d511f46/src/perl/vcf-contrast\n",
      "vcftools-vcftools-d511f46/src/perl/vcf-convert\n",
      "vcftools-vcftools-d511f46/src/perl/vcf-fix-newlines\n",
      "vcftools-vcftools-d511f46/src/perl/vcf-fix-ploidy\n",
      "vcftools-vcftools-d511f46/src/perl/vcf-haplotypes\n",
      "vcftools-vcftools-d511f46/src/perl/vcf-indel-stats\n",
      "vcftools-vcftools-d511f46/src/perl/vcf-isec\n",
      "vcftools-vcftools-d511f46/src/perl/vcf-merge\n",
      "vcftools-vcftools-d511f46/src/perl/vcf-phased-join\n",
      "vcftools-vcftools-d511f46/src/perl/vcf-query\n",
      "vcftools-vcftools-d511f46/src/perl/vcf-shuffle-cols\n",
      "vcftools-vcftools-d511f46/src/perl/vcf-sort\n",
      "vcftools-vcftools-d511f46/src/perl/vcf-stats\n",
      "vcftools-vcftools-d511f46/src/perl/vcf-subset\n",
      "vcftools-vcftools-d511f46/src/perl/vcf-to-tab\n",
      "vcftools-vcftools-d511f46/src/perl/vcf-tstv\n",
      "vcftools-vcftools-d511f46/src/perl/vcf-validator\n",
      "mv: cannot move 'vcftools/' to a subdirectory of itself, '/home/jupyter/vcftools/vcftools'\n",
      "mv: cannot move 'vcftools-vcftools-d511f46/' to '/home/jupyter/vcftools/vcftools-vcftools-d511f46': Directory not empty\n",
      "checking for a BSD-compatible install... /usr/bin/install -c\n",
      "checking whether build environment is sane... yes\n",
      "checking for a thread-safe mkdir -p... /usr/bin/mkdir -p\n",
      "checking for gawk... no\n",
      "checking for mawk... mawk\n",
      "checking whether make sets $(MAKE)... yes\n",
      "checking whether make supports nested variables... yes\n",
      "checking for g++... g++\n",
      "checking whether the C++ compiler works... yes\n",
      "checking for C++ compiler default output file name... a.out\n",
      "checking for suffix of executables... \n",
      "checking whether we are cross compiling... no\n",
      "checking for suffix of object files... o\n",
      "checking whether we are using the GNU C++ compiler... yes\n",
      "checking whether g++ accepts -g... yes\n",
      "checking whether make supports the include directive... yes (GNU style)\n",
      "checking dependency style of g++... gcc3\n",
      "checking for gcc... gcc\n",
      "checking whether we are using the GNU C compiler... yes\n",
      "checking whether gcc accepts -g... yes\n",
      "checking for gcc option to accept ISO C89... none needed\n",
      "checking whether gcc understands -c and -o together... yes\n",
      "checking dependency style of gcc... gcc3\n",
      "checking how to run the C preprocessor... gcc -E\n",
      "checking for perl... /usr/bin/perl\n",
      "checking for pkg-config... /usr/bin/pkg-config\n",
      "checking pkg-config is at least version 0.9.0... yes\n",
      "checking for ZLIB... yes\n",
      "checking for grep that handles long lines and -e... /usr/bin/grep\n",
      "checking for egrep... /usr/bin/grep -E\n",
      "checking for ANSI C header files... yes\n",
      "checking for sys/types.h... yes\n",
      "checking for sys/stat.h... yes\n",
      "checking for stdlib.h... yes\n",
      "checking for string.h... yes\n",
      "checking for memory.h... yes\n",
      "checking for strings.h... yes\n",
      "checking for inttypes.h... yes\n",
      "checking for stdint.h... yes\n",
      "checking for unistd.h... yes\n",
      "checking arpa/inet.h usability... yes\n",
      "checking arpa/inet.h presence... yes\n",
      "checking for arpa/inet.h... yes\n",
      "checking fcntl.h usability... yes\n",
      "checking fcntl.h presence... yes\n",
      "checking for fcntl.h... yes\n",
      "checking limits.h usability... yes\n",
      "checking limits.h presence... yes\n",
      "checking for limits.h... yes\n",
      "checking netdb.h usability... yes\n",
      "checking netdb.h presence... yes\n",
      "checking for netdb.h... yes\n",
      "checking for stdint.h... (cached) yes\n",
      "checking for stdlib.h... (cached) yes\n",
      "checking for string.h... (cached) yes\n",
      "checking sys/socket.h usability... yes\n",
      "checking sys/socket.h presence... yes\n",
      "checking for sys/socket.h... yes\n",
      "checking for unistd.h... (cached) yes\n",
      "checking for stdbool.h that conforms to C99... yes\n",
      "checking for _Bool... yes\n",
      "checking for inline... inline\n",
      "checking for int16_t... yes\n",
      "checking for int32_t... yes\n",
      "checking for int64_t... yes\n",
      "checking for int8_t... yes\n",
      "checking for off_t... yes\n",
      "checking for size_t... yes\n",
      "checking for ssize_t... yes\n",
      "checking for uint16_t... yes\n",
      "checking for uint32_t... yes\n",
      "checking for uint8_t... yes\n",
      "checking for special C compiler options needed for large files... no\n",
      "checking for _FILE_OFFSET_BITS value needed for large files... no\n",
      "checking for error_at_line... yes\n",
      "checking for gethostbyaddr... yes\n",
      "checking for gethostbyname... yes\n",
      "checking for malloc... yes\n",
      "checking for memset... yes\n",
      "checking for pow... no\n",
      "checking for realloc... yes\n",
      "checking for select... yes\n",
      "checking for socket... yes\n",
      "checking for sqrt... no\n",
      "checking for strchr... yes\n",
      "checking for strdup... yes\n",
      "checking for strerror... yes\n",
      "checking for strstr... yes\n",
      "checking for strtol... yes\n",
      "checking that generated files are newer than configure... done\n",
      "configure: creating ./config.status\n",
      "config.status: creating Makefile\n",
      "config.status: creating src/Makefile\n",
      "config.status: creating src/cpp/Makefile\n",
      "config.status: creating src/perl/Makefile\n",
      "config.status: creating config.h\n",
      "config.status: config.h is unchanged\n",
      "config.status: executing depfiles commands\n",
      "make  all-recursive\n",
      "make[1]: Entering directory '/home/jupyter/vcftools'\n",
      "Making all in src\n",
      "make[2]: Entering directory '/home/jupyter/vcftools/src'\n",
      "Making all in cpp\n",
      "make[3]: Entering directory '/home/jupyter/vcftools/src/cpp'\n",
      "make[3]: Nothing to be done for 'all'.\n",
      "make[3]: Leaving directory '/home/jupyter/vcftools/src/cpp'\n",
      "Making all in perl\n",
      "make[3]: Entering directory '/home/jupyter/vcftools/src/perl'\n",
      "make[3]: Nothing to be done for 'all'.\n",
      "make[3]: Leaving directory '/home/jupyter/vcftools/src/perl'\n",
      "make[3]: Entering directory '/home/jupyter/vcftools/src'\n",
      "make[3]: Nothing to be done for 'all-am'.\n",
      "make[3]: Leaving directory '/home/jupyter/vcftools/src'\n",
      "make[2]: Leaving directory '/home/jupyter/vcftools/src'\n",
      "make[2]: Entering directory '/home/jupyter/vcftools'\n",
      "make[2]: Leaving directory '/home/jupyter/vcftools'\n",
      "make[1]: Leaving directory '/home/jupyter/vcftools'\n",
      "Making install in src\n",
      "make[1]: Entering directory '/home/jupyter/vcftools/src'\n",
      "Making install in cpp\n",
      "make[2]: Entering directory '/home/jupyter/vcftools/src/cpp'\n",
      "make[3]: Entering directory '/home/jupyter/vcftools/src/cpp'\n",
      " /usr/bin/mkdir -p '/home/jupyter/bin'\n",
      "  /usr/bin/install -c vcftools '/home/jupyter/bin'\n",
      " /usr/bin/mkdir -p '/home/jupyter/share/man/man1'\n",
      " /usr/bin/install -c -m 644 vcftools.1 '/home/jupyter/share/man/man1'\n",
      "make[3]: Leaving directory '/home/jupyter/vcftools/src/cpp'\n",
      "make[2]: Leaving directory '/home/jupyter/vcftools/src/cpp'\n",
      "Making install in perl\n",
      "make[2]: Entering directory '/home/jupyter/vcftools/src/perl'\n",
      "make[3]: Entering directory '/home/jupyter/vcftools/src/perl'\n",
      " /usr/bin/mkdir -p '/home/jupyter/bin'\n",
      " /usr/bin/install -c fill-aa fill-an-ac fill-fs fill-ref-md5 vcf-annotate vcf-compare vcf-concat vcf-consensus vcf-contrast vcf-convert vcf-fix-newlines vcf-fix-ploidy vcf-indel-stats vcf-isec vcf-merge vcf-phased-join vcf-query vcf-shuffle-cols vcf-sort vcf-stats vcf-subset vcf-to-tab vcf-tstv vcf-validator '/home/jupyter/bin'\n",
      " /usr/bin/mkdir -p '/home/jupyter/share/perl/5.30.0'\n",
      " /usr/bin/install -c -m 644 FaSlice.pm Vcf.pm VcfStats.pm '/home/jupyter/share/perl/5.30.0'\n",
      "make[3]: Leaving directory '/home/jupyter/vcftools/src/perl'\n",
      "make[2]: Leaving directory '/home/jupyter/vcftools/src/perl'\n",
      "make[2]: Entering directory '/home/jupyter/vcftools/src'\n",
      "make[3]: Entering directory '/home/jupyter/vcftools/src'\n",
      "make[3]: Nothing to be done for 'install-exec-am'.\n",
      "make[3]: Nothing to be done for 'install-data-am'.\n",
      "make[3]: Leaving directory '/home/jupyter/vcftools/src'\n",
      "make[2]: Leaving directory '/home/jupyter/vcftools/src'\n",
      "make[1]: Leaving directory '/home/jupyter/vcftools/src'\n",
      "make[1]: Entering directory '/home/jupyter/vcftools'\n",
      "make[2]: Entering directory '/home/jupyter/vcftools'\n",
      "make[2]: Nothing to be done for 'install-exec-am'.\n",
      "make[2]: Nothing to be done for 'install-data-am'.\n",
      "make[2]: Leaving directory '/home/jupyter/vcftools'\n",
      "make[1]: Leaving directory '/home/jupyter/vcftools'\n"
     ]
    }
   ],
   "source": [
    "# install vcftools and complete setup\n",
    "# don't worry about the pyvcf error \n",
    "\n",
    "!bash ~/setup.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpful functions\n",
    "\n",
    "def truncate(s, first_few, last_few):\n",
    "    \"truncate string printing only first_few and last_few characters\"\n",
    "    return f\"{s[:first_few]}...{s[-last_few:]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get [1000G](https://anvil.terra.bio/#workspaces/anvil-datastorage/AnVIL_1000G_PRIMED-data-model/data) VCF Data for NA12878"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify patient and chromosomes\n",
    "\n",
    "chrs_of_interest = set(\"1\")\n",
    "patient = \"NA12878\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity:sequencing_file_id</th>\n",
       "      <th>chromosome</th>\n",
       "      <th>file_path</th>\n",
       "      <th>file_type</th>\n",
       "      <th>md5sum</th>\n",
       "      <th>sequencing_dataset_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00029d9f</td>\n",
       "      <td>4</td>\n",
       "      <td>gs://fc-2ee2ca2a-a140-48a1-b793-e27badb7945d/p...</td>\n",
       "      <td>PLINK2 pvar</td>\n",
       "      <td>9e193baa707fa07a007ed1f6595d8427</td>\n",
       "      <td>JPT_hg19_mega_hm3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000d7475</td>\n",
       "      <td>7</td>\n",
       "      <td>gs://fc-2ee2ca2a-a140-48a1-b793-e27badb7945d/p...</td>\n",
       "      <td>PLINK2 psam</td>\n",
       "      <td>1c0ce61414556e3f00b132a455d6132b</td>\n",
       "      <td>FIN_hg38_mega_hm3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00131101</td>\n",
       "      <td>14</td>\n",
       "      <td>gs://fc-2ee2ca2a-a140-48a1-b793-e27badb7945d/p...</td>\n",
       "      <td>PLINK2 psam</td>\n",
       "      <td>02fafde1613a0dae38ac0c0ef30da925</td>\n",
       "      <td>GIH_hg38_mega_hm3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0018383c</td>\n",
       "      <td>10</td>\n",
       "      <td>gs://fc-2ee2ca2a-a140-48a1-b793-e27badb7945d/p...</td>\n",
       "      <td>PLINK2 pgen</td>\n",
       "      <td>ad2cc5b44ad6d75d2fa1a420227214cd</td>\n",
       "      <td>ASW_hg38_hm3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00192ae2</td>\n",
       "      <td>11</td>\n",
       "      <td>gs://fc-2ee2ca2a-a140-48a1-b793-e27badb7945d/p...</td>\n",
       "      <td>PLINK2 pgen</td>\n",
       "      <td>fa2e3a6475bc43ab3bede56cf4b3f5f3</td>\n",
       "      <td>AMR_hg19_hm3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity:sequencing_file_id chromosome  \\\n",
       "0                  00029d9f          4   \n",
       "1                  000d7475          7   \n",
       "2                  00131101         14   \n",
       "3                  0018383c         10   \n",
       "4                  00192ae2         11   \n",
       "\n",
       "                                           file_path    file_type  \\\n",
       "0  gs://fc-2ee2ca2a-a140-48a1-b793-e27badb7945d/p...  PLINK2 pvar   \n",
       "1  gs://fc-2ee2ca2a-a140-48a1-b793-e27badb7945d/p...  PLINK2 psam   \n",
       "2  gs://fc-2ee2ca2a-a140-48a1-b793-e27badb7945d/p...  PLINK2 psam   \n",
       "3  gs://fc-2ee2ca2a-a140-48a1-b793-e27badb7945d/p...  PLINK2 pgen   \n",
       "4  gs://fc-2ee2ca2a-a140-48a1-b793-e27badb7945d/p...  PLINK2 pgen   \n",
       "\n",
       "                             md5sum sequencing_dataset_id  \n",
       "0  9e193baa707fa07a007ed1f6595d8427     JPT_hg19_mega_hm3  \n",
       "1  1c0ce61414556e3f00b132a455d6132b     FIN_hg38_mega_hm3  \n",
       "2  02fafde1613a0dae38ac0c0ef30da925     GIH_hg38_mega_hm3  \n",
       "3  ad2cc5b44ad6d75d2fa1a420227214cd          ASW_hg38_hm3  \n",
       "4  fa2e3a6475bc43ab3bede56cf4b3f5f3          AMR_hg19_hm3  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get metadata for filepaths\n",
    "# openly sourced from https://anvil.terra.bio/#workspaces/anvil-datastorage/AnVIL_1000G_PRIMED-data-model/data\n",
    "\n",
    "df = pd.read_csv(io.StringIO(fapi.get_entities_tsv(\"anvil-datastorage\", \\\n",
    "                \"AnVIL_1000G_PRIMED-data-model\", \"sequencing_file\", model=\"flexible\").text), sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of gvcf data\n",
    "\n",
    "df_vcf = df[df['file_type'].isin(['VCF', 'VCF index'])]\n",
    "df_1kgp = df_vcf[df_vcf['file_path'].str.contains('1kGP')]\n",
    "\n",
    "num_vcf_idx_files = sum(df_1kgp['file_type'] == 'VCF index')\n",
    "num_vcf_files = sum(df_1kgp['file_type'] == 'VCF')\n",
    "assert num_vcf_files == 23 and num_vcf_idx_files == 23, \\\n",
    "    f\"check number of files, {num_vcf_files} vcfs and {num_vcf_idx_files} index files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1kGP_high_coverage_Illumina.chr1.fi...vcf.gz.tbi already exists, not downloading\n",
      "1kGP_high_coverage_Illumina.chr1.fi...nel.vcf.gz already exists, not downloading\n"
     ]
    }
   ],
   "source": [
    "# load 1000G file if doesn't exist\n",
    "\n",
    "df_chrs = df_1kgp[df_1kgp['chromosome'].isin(chrs_of_interest)]\n",
    "uris = df_chrs['file_path']\n",
    "file_names = [uri.split(\"/\")[-1] for uri in uris]\n",
    "\n",
    "for file_name, uri in zip(file_names, uris):\n",
    "    if os.path.exists(f\"{os.environ['INPUT_DIR']}/{file_name}\"):\n",
    "        print(f\"{truncate(file_name, 35, 10)} already exists, not downloading\")\n",
    "    else:\n",
    "        split_vcf_cmd = f\"gsutil -u $GOOGLE_PROJECT cp {uri} $INPUT_DIR/\"\n",
    "        output = subprocess.run(split_vcf_cmd, shell=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/vcf/1kGP_high_coverage_Illumina.chr1.filtered.SNV_INDEL_SV_phased_panel.vcf.gz.tbi\n"
     ]
    }
   ],
   "source": [
    "vcfs = df_chrs[df_chrs['file_type'] == 'VCF']\n",
    "input_vcf = f\"{os.environ['INPUT_DIR']}/{file_names[0]}\"\n",
    "\n",
    "print(input_vcf)\n",
    "assert os.path.exists(input_vcf), \"file doesn't exist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 565M\r\n",
      "-rw-rw-r-- 1 jupyter users  167 Feb 23 06:16 NA12878-0-to-169407-hits.pkl\r\n",
      "-rw-rw-r-- 1 jupyter users  48K Feb 22 23:00 NA12878.1000.vcf\r\n",
      "-rw-rw-r-- 1 jupyter users  57K Feb 22 23:01 NA12878.1000.vcf.gz\r\n",
      "-rw-rw-r-- 1 jupyter users 695K Feb 22 23:01 NA12878.1000-vrs-objects.pkl\r\n",
      "-rw-rw-r-- 1 jupyter users  167 Feb 23 08:26 NA12878-169407-to-338815-hits.pkl\r\n",
      "-rw-rw-r-- 1 jupyter users  16M Feb 22 01:38 NA12878.filtered.vcf\r\n",
      "-rw-rw-r-- 1 jupyter users  21M Feb 23 00:06 NA12878.filtered.vcf.gz\r\n",
      "-rw-rw-r-- 1 jupyter users 264M Feb 23 00:06 NA12878.filtered-vrs-objects.pkl\r\n",
      "-rw-rw-r-- 1 jupyter users 264M Feb 22 01:06 NA12878.recode.vcf\r\n"
     ]
    }
   ],
   "source": [
    "! ls -lh /home/jupyter/split/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 565M\r\n",
      "-rw-rw-r-- 1 jupyter users  167 Feb 23 06:16 NA12878-0-to-169407-hits.pkl\r\n",
      "-rw-rw-r-- 1 jupyter users  48K Feb 22 23:00 NA12878.1000.vcf\r\n",
      "-rw-rw-r-- 1 jupyter users  57K Feb 22 23:01 NA12878.1000.vcf.gz\r\n",
      "-rw-rw-r-- 1 jupyter users 695K Feb 22 23:01 NA12878.1000-vrs-objects.pkl\r\n",
      "-rw-rw-r-- 1 jupyter users  167 Feb 23 08:26 NA12878-169407-to-338815-hits.pkl\r\n",
      "-rw-rw-r-- 1 jupyter users  16M Feb 22 01:38 NA12878.chr1.filtered.vcf\r\n",
      "-rw-rw-r-- 1 jupyter users 264M Feb 22 01:06 NA12878.chr1.recode.vcf\r\n",
      "-rw-rw-r-- 1 jupyter users  21M Feb 23 00:06 NA12878.filtered.vcf.gz\r\n",
      "-rw-rw-r-- 1 jupyter users 264M Feb 23 00:06 NA12878.filtered-vrs-objects.pkl\r\n"
     ]
    }
   ],
   "source": [
    "! mv /home/jupyter/split/NA12878.filtered.vcf /home/jupyter/split/NA12878.chr1.filtered.vcf\n",
    "! mv /home/jupyter/split/NA12878.recode.vcf /home/jupyter/split/NA12878.chr1.recode.vcf\n",
    "! ls -lh /home/jupyter/split/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already split file: /home/jupyter/split/NA12878.chr1.recode.vcf\n",
      "already filtered file: /home/jupyter/split/NA12878.chr1.filtered.vcf\n"
     ]
    }
   ],
   "source": [
    "# get patient-level for single chr\n",
    "patient_path_stem = f\"{os.environ['SPLIT_DIR']}/{patient}\" \n",
    "\n",
    "for c in chrs_of_interest:\n",
    "    patient_vcf_path = f\"{patient_path_stem}.chr{c}.recode.vcf\"\n",
    "    filtered_patient_vcf_path = f\"{patient_path_stem}.chr{c}.filtered.vcf\"\n",
    "\n",
    "    # split vcf by patient\n",
    "    if os.path.exists(patient_vcf_path):\n",
    "        print(f\"already split file: {patient_vcf_path}\")\n",
    "    else:\n",
    "        split_vcf_cmd = f\"$VCFTOOLS --recode --gzvcf {input_vcf} \\\n",
    "                    --out {patient_vcf_path} --indv {patient}\"\n",
    "\n",
    "        output = subprocess.run(split_vcf_cmd, shell=True, check=True)\n",
    "        \n",
    "    # filter to only relevant genotypes\n",
    "    if os.path.exists(filtered_patient_vcf_path):\n",
    "        print(f\"already filtered file: {filtered_patient_vcf_path}\")\n",
    "    else:\n",
    "        filter_genotypes_cmd = f'grep -v \"0|0\" $SPLIT_DIR/{patient}.recode.vcf' + \\\n",
    "                        f\" > {filtered_patient_vcf_path}\"\n",
    "        subprocess.run(filter_genotypes_cmd, shell=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OPTIONAL\n",
    "# # filter to first num_lines\n",
    "\n",
    "# num_lines = 1000\n",
    "# head_vcf = f\"{patient_path_stem}.{num_lines}.vcf\"\n",
    "\n",
    "# head_cmd = f\"cat {filtered_patient_vcf_path} | head -n {num_lines} > {head_vcf}\"\n",
    "# output = subprocess.run(head_cmd, shell=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !find ~ -name *1000.vcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338926 /home/jupyter/split/NA12878.filtered.vcf\r\n"
     ]
    }
   ],
   "source": [
    "# get number of lines in \n",
    "! wc -l $SPLIT_DIR/NA12878.filtered.vcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# checking my work, make sure it totals up to the final number\n",
    "\n",
    "for term in [\"0|1\", \"1|0\", \"1|1\", \"0|0\", \"#\", \".\"]:\n",
    "    subprocess.run(f\"grep '{term}' $SPLIT_DIR/NA12878.recode.vcf | wc -l\", \\\n",
    "                  shell=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create annotated vcf test file \n",
    "# def annotate_vcf(input_vcf, output_vcf, output_pkl, seqrepo_root_dir, require_validation=True, rle_seq_limit=50):\n",
    "#     '''param stem: path of input vcf file'''\n",
    "#     vcf_annotator = VCFAnnotator(seqrepo_root_dir=seqrepo_root_dir)\n",
    "#     vcf_annotator.tlr.rle_seq_limit = rle_seq_limit\n",
    "#     vcf_annotator.annotate(vcf_in=input_vcf, vcf_out=output_vcf, \\\n",
    "#         vrs_pickle_out=output_pkl, require_validation=require_validation)\n",
    "\n",
    "# logger = logging.getLogger(\"ga4gh.vrs.extras.vcf_annotation\")\n",
    "# logger.setLevel(level=logging.INFO)\n",
    "\n",
    "# curr_input_vcf = filtered_patient_vcf_path\n",
    "\n",
    "# stem = curr_input_vcf.replace('.vcf', '')\n",
    "# output_vcf = f\"{stem}.vcf.gz\"\n",
    "# output_pkl = f\"{stem}-vrs-objects.pkl\"\n",
    "\n",
    "# print(\"writing to...\")\n",
    "# print(output_vcf)\n",
    "# print(output_pkl)\n",
    "\n",
    "# t = time()\n",
    "# annotate_vcf(curr_input_vcf, output_vcf, output_pkl, SEQREPO_DIR)\n",
    "# elapsed_time = time()-t\n",
    "# print(f\"annotation: {(elapsed_time):.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     71
    ]
   },
   "outputs": [],
   "source": [
    "def unpickle(file_name):\n",
    "    \"\"\"Unpickle vrs objects to single dict\"\"\"\n",
    "    with open(file_name, 'rb') as f:\n",
    "        vrs_objects = pickle.load(f)\n",
    "        for k, v in vrs_objects.items():\n",
    "            vrs_objects[k] = ast.literal_eval(v)\n",
    "    \n",
    "    return vrs_objects\n",
    "\n",
    "def meta_kb(id, recent=True, log=False):\n",
    "    \"\"\"Query metakb using vrs object\"\"\"\n",
    "    # k, allele_dict = item\n",
    "    \n",
    "    # if translator is not None:\n",
    "    #     if log: print(f\"by {fmt}...\")\n",
    "    #     allele = models.Allele(**allele_dict)\n",
    "    #     _id = translator.translate_to(allele, fmt)\n",
    "    # else:\n",
    "    #     if log: print(\"by vrs id...\")\n",
    "    #     _id = allele_dict[\"id\"]\n",
    "        \n",
    "        \n",
    "    if recent:\n",
    "        if log: print(\"recent elasticbeanstalk api (VRS 2.0 models)\")\n",
    "        response = requests.get(\"http://metakb-dev-eb.us-east-2.elasticbeanstalk.com\" \\\n",
    "                                f\"/api/v2/search/studies?variation={id}&detail=false\")\n",
    "    else:\n",
    "        if log: print(\"old api (VRS 1.3 models)\")\n",
    "        response = requests.get(\"https://dev-search.cancervariants.org\" \\\n",
    "                                f\"/api/v2/search?variation={id}&detail=false\")\n",
    "    \n",
    "    response_json = response.json()\n",
    "    \n",
    "    if response_json['warnings'] == []:\n",
    "        return (id, response_json)\n",
    "    else:\n",
    "        if log: print(response_json['warnings'])\n",
    "\n",
    "def num_variants(input_vcf):\n",
    "    # get total num_variants\n",
    "    vcf_reader = pysam.VariantFile(open(input_vcf, 'r'))\n",
    "    return sum(1 for _ in vcf_reader)\n",
    "\n",
    "def parallelize(vrs_decorator, vrs_objects, worker_count=4, progress_interval=500, limit=None):\n",
    "    \"\"\"harvest data from service\"\"\"\n",
    "\n",
    "    manager = multiprocessing.Manager()\n",
    "    results = manager.list()\n",
    "\n",
    "    with multiprocessing.Pool(worker_count) as pool:\n",
    "        # call the function for each item in parallel\n",
    "        c = 0\n",
    "        print(datetime.now().isoformat(), c)\n",
    "\n",
    "        for result in pool.imap(vrs_decorator, vrs_objects):\n",
    "            c += 1\n",
    "            if result:\n",
    "                results.append(result)\n",
    "            if c == limit:\n",
    "                break\n",
    "            elif c % progress_interval == 0:\n",
    "                print(datetime.now().isoformat(), c)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_dict(d, indent=2):\n",
    "    \"\"\"pretty print object as json\"\"\"\n",
    "    print(json.dumps(d, indent=indent))\n",
    "\n",
    "def print_percent(a, b):\n",
    "    \"pretty print percentages\"\n",
    "    print(f\"{a}/{b} = {(100.0*a/b):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get total num_variants\n",
    "curr_input_vcf = filtered_patient_vcf_path\n",
    "\n",
    "vcf_reader = pysam.VariantFile(open(curr_input_vcf, 'r'))\n",
    "num_variants = int(subprocess.run(f\"grep -v '^#' {curr_input_vcf} | wc -l\", \\\n",
    "                              stdout=subprocess.PIPE, shell=True, \\\n",
    "                              check=True, text=True).stdout)\n",
    "\n",
    "print(f'num_vrs_objects to num_variants: {len(allele_dicts)}/{num_variants}={100*(len(allele_dicts)/num_variants):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/jupyter/split/NA12878-0-to-169407-hits.pkl\", 'rb') as file:\n",
    "    thing = pickle.load(file)\n",
    "    for t in thing:\n",
    "        print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "id_start = num_variants\n",
    "id_end = num_variants*2 # ids to process\n",
    "progress_interval = 10000\n",
    "metakb_output_pkl = f\"{patient_path_stem}-{id_start}-to-{id_end}-hits.pkl\"\n",
    "\n",
    "print(f\"writing to {metakb_output_pkl}\")\n",
    "\n",
    "stem = curr_input_vcf.replace('.vcf', '')\n",
    "output_vcf = f\"{stem}.vcf.gz\"\n",
    "output_pkl = f\"{stem}-vrs-objects.pkl\"\n",
    "    \n",
    "# get pickle totals\n",
    "allele_dicts = unpickle(output_pkl)\n",
    "\n",
    "# convert alleles to vrs ids\n",
    "t = time()\n",
    "vrs_ids = [ga4gh_identify(models.Allele(**allele_dict)) \\\n",
    "            for i, (_, allele_dict) in enumerate(allele_dicts.items()) \\\n",
    "            if i >= id_start and i < id_end]\n",
    "print(f\"{id_end-id_start} ids: {(time()-t):.2f} s\")\n",
    "\n",
    "# number of workers\n",
    "worker_count = 4*os.cpu_count()\n",
    "\n",
    "# ping metakb\n",
    "print(\"pinging metakb...\")\n",
    "t = time()\n",
    "hits = parallelize(meta_kb, vrs_ids, worker_count=worker_count, \\\n",
    "    progress_interval=progress_interval)\n",
    "print(f\"metakb: {(time()-t):.2f} s\")\n",
    "\n",
    "with open(metakb_output_pkl, 'wb') as file:\n",
    "    pickle.dump(hits, file)\n",
    "\n",
    "print(\"\\nhits to ids queried...\")\n",
    "total = num_ids_limit if num_ids_limit else len(vrs_ids)\n",
    "print_percent(len(hits), total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Split before annotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! (seq 1 22; echo X; echo Y) | xargs -P 0 -I PATH $VCFTOOLS --recode --vcf \"/home/jupyter/vcf/1KGP_haplotype_caller_NA12878.chr10.hc.vcf\" --chr chrPATH --out $SPLIT_DIR/chrPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vcf_path = drs_vcfs[0]\n",
    "\n",
    "! rm -r $SPLIT_DIR\n",
    "split_vcf_cmd = f\"(seq 1 22; echo X; echo Y) | \\\n",
    "               xargs -P 0 -I PATH $VCFTOOLS --recode --gzvcf {vcf_path} \\\n",
    "               --chr chrPATH --out $SPLIT_DIR/chrPATH\"\n",
    "\n",
    "output = subprocess.run(split_vcf_cmd, shell=True, check=True)\n",
    "# output = subprocess.run(split_vcf_cmd, shell=True, check=True, \\\n",
    "#                         capture_output=True, text=True) \n",
    "\n",
    "# no chr prefix\n",
    "# ! (seq 1 22; echo X; echo Y) | xargs -P 0 -I PATH ~/vcftools-vcftools-d511f46/src/cpp/vcftools --recode --vcf $VCF_PATH --chr PATH --out ~/split/chrPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: parse logs to get outputs on how many were filtered out\n",
    "# get total num_variants\n",
    "\n",
    "def get_num_variants(path):\n",
    "    vcf_reader = pysam.VariantFile(open(path, 'r'))\n",
    "    return sum(1 for record in vcf_reader)\n",
    "\n",
    "split_vcf_paths = glob.glob(f\"{os.environ.get('SPLIT_DIR')}/*.recode.vcf\")\n",
    "     \n",
    "input_num_variants = get_num_variants(vcf_path[:-3])\n",
    "split_num_variants = sum(get_num_variants(path) for path in split_vcf_paths)\n",
    "\n",
    "print(f\"{split_num_variants}/{input_num_variants} = \", \\\n",
    "      f\"{100*split_num_variants/input_num_variants:.2f}% kept\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ls -l $SPLIT_DIR/*.recode.vcf | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# annotate each of them\n",
    "# TODO: fix the outputs coming from this\n",
    "\n",
    "! (ls -1 $SPLIT_DIR/*.recode.vcf | \\\n",
    "   xargs -P 0 -I PATH python3 -m ga4gh.vrs.extras.vcf_annotation \\\n",
    "   --vcf_in PATH --vcf_out PATH.vcf.gz --vrs_pickle_out PATH.pkl \\\n",
    "   --seqrepo_root_dir $SEQREPO_ROOT/latest \\\n",
    "   2> $SPLIT_DIR/chrPATH_log.txt)\n",
    "\n",
    "# # GREGoR\n",
    "# !python3 -m ga4gh.vrs.extras.vcf_annotation --vcf_in 1369747.merged.matefixed.sorted.markeddups.recal.g.vcf  --vcf_out 1369747.merged.matefixed.sorted.markeddups.recal.g.vcf.output.vcf.gz --vrs_pickle_out 1369747.merged.matefixed.sorted.markeddups.recal.g.vcf.vrs_objects.pkl  --seqrepo_root_dir ~/seqrepo/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls -l $SPLIT_DIR/*.vcf.vcf.gz | wc -l\n",
    "!ls -l $SPLIT_DIR/*.vcf.pkl | wc -l\n",
    "\n",
    "# assert (!ls -l ~/split/*.vcf.vcf.gz | wc -l) == 24, \"incorrect number of output vcf.gz files created\"\n",
    "# assert (!ls -l ~/split/*.vcf.pkl | wc -l) == 24, \"incorrect number of outputted pickle files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# join the files\n",
    "!ls -1 $SPLIT_DIR/*.vcf.vcf.gz | xargs $PERL5LIB/vcf-concat > $OUTPUT/merged_output.vcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls $OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: remove the pair of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Random python annotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"ga4gh.vrs.extras.vcf_annotation\")\n",
    "logger.setLevel(level=logging.INFO)\n",
    "\n",
    "# create annotated vcf test file \n",
    "def annotate_vcf(path):\n",
    "    '''param stem: path of input vcf file'''\n",
    "    stem = path.replace(\".vcf\", \"\")\n",
    "    \n",
    "    input_vcf = path\n",
    "    output_vcf = f\"{stem}.output.vcf.gz\"\n",
    "    output_pkl = f\"{stem}-vrs-objects.pkl\"\n",
    "\n",
    "    \n",
    "    vcf_annotator = VCFAnnotator(seqrepo_root_dir=\"/home/jupyter/seqrepo/latest\")\n",
    "    vcf_annotator.annotate(vcf_in=input_vcf, vcf_out=output_vcf, vrs_pickle_out=output_pkl)\n",
    "    # vcf_annotator.annotate(vcf_in=input_vcf, vrs_pickle_out=output_pkl)\n",
    "    \n",
    "# annotate_vcf(\"/home/jupyter/split\", \"chr1.recode\")\n",
    "successes = set()\n",
    "for vcf_path in drs_vcfs:\n",
    "    try:\n",
    "        print(\"trying...\", vcf_path)\n",
    "        annotate_vcf(vcf_path)\n",
    "        print(\"worked \\n\")\n",
    "        successes.add(vcf_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"unsucessful, see logs above \\n\")\n",
    "\n",
    "print(f\"total successes: {len(successes)}/{len(drs_vcfs)} \\nList...\")\n",
    "for vcf_path in drs_vcfs:\n",
    "    print(f\"{vcf_path}: {'✓' if vcf_path in successes else 'x'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# annotate w vrs id asking for output vcf\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"ga4gh.vrs.extras.vcf_annotation\")\n",
    "logger.setLevel(level=logging.INFO)\n",
    "\n",
    "# create annotated vcf test file \n",
    "def annotate_vcf(path):\n",
    "    '''param stem: path of input vcf file'''\n",
    "    stem = path.replace(\".vcf\", \"\")\n",
    "    \n",
    "    input_vcf = path\n",
    "    output_vcf = f\"{stem}.output.vcf.gz\"\n",
    "    output_pkl = f\"{stem}-vrs-objects.pkl\"\n",
    "\n",
    "    \n",
    "    vcf_annotator = VCFAnnotator(seqrepo_root_dir=\"/home/jupyter/seqrepo/latest\")\n",
    "    vcf_annotator.annotate(vcf_in=input_vcf, vcf_out=output_vcf, vrs_pickle_out=output_pkl)\n",
    "    # vcf_annotator.annotate(vcf_in=input_vcf, vrs_pickle_out=output_pkl)\n",
    "    \n",
    "# annotate_vcf(\"/home/jupyter/split\", \"chr1.recode\")\n",
    "successes = set()\n",
    "for vcf_path in drs_vcfs:\n",
    "    try:\n",
    "        print(\"trying...\", vcf_path)\n",
    "        annotate_vcf(vcf_path)\n",
    "        print(\"worked \\n\")\n",
    "        successes.add(vcf_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"unsucessful, see logs above \\n\")\n",
    "\n",
    "print(f\"total successes: {len(successes)}/{len(drs_vcfs)} \\nList...\")\n",
    "for vcf_path in drs_vcfs:\n",
    "    print(f\"{vcf_path}: {'✓' if vcf_path in successes else 'x'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for vcf_path in drs_vcfs:\n",
    "    if \"HG02080vCHM13_20200921\" in vcf_path:\n",
    "        print(vcf_path)\n",
    "    else:\n",
    "        continue\n",
    "#     if \"chm13_hifi_HG007\" in vcf_path:\n",
    "#         print(\"trying...\", vcf_path)\n",
    "#         annotate_vcf(vcf_path)\n",
    "#         print(\"worked \\n\")\n",
    "    try:\n",
    "        print(\"trying...\", vcf_path)\n",
    "        annotate_vcf(vcf_path)\n",
    "        print(\"worked \\n\")\n",
    "        successes.add(vcf_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"unsucessful, see logs above \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# annotate w vrs id only pickle outputted\n",
    "\n",
    "logger = logging.getLogger(\"ga4gh.vrs.extras.vcf_annotation\")\n",
    "# logger.setLevel(level=logging.ERROR)\n",
    "logger.disabled = True\n",
    "\n",
    "# create annotated vcf test file \n",
    "def annotate_vcf_pkl_only(path):\n",
    "    '''param stem: path of input vcf file'''\n",
    "    stem = path.replace(\".vcf\", \"\")\n",
    "    \n",
    "    input_vcf = path\n",
    "    output_vcf = f\"{stem}.output.vcf.gz\"\n",
    "    output_pkl = f\"{stem}-vrs-objects.pkl\"\n",
    "\n",
    "    \n",
    "    vcf_annotator = VCFAnnotator(seqrepo_root_dir=\"/home/jupyter/seqrepo/latest\")\n",
    "    vcf_annotator.annotate(vcf_in=input_vcf, vrs_pickle_out=output_pkl)\n",
    "    \n",
    "successes = set()\n",
    "for i, vcf_path in enumerate(drs_vcfs):\n",
    "    print(\"starting... \\n\")\n",
    "    # annotate to output pkl\n",
    "    try:\n",
    "        print(\"trying...\", vcf_path)\n",
    "        annotate_vcf_pkl_only(vcf_path)\n",
    "        print(\"worked \\n\")\n",
    "        successes.add(vcf_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"unsucessful, see logs above \\n\")\n",
    "    \n",
    "    # get pickle totals\n",
    "    try:\n",
    "        with open(output_pkl, 'rb') as f:\n",
    "            vrs_objects = pickle.load(f)\n",
    "\n",
    "        # get total num_variants\n",
    "        vcf_reader = vcf.Reader(open(vcf_path, 'r'))\n",
    "        num_variants = sum(1 for record in vcf_reader)\n",
    "\n",
    "        # view details\n",
    "        print(f'num_vrs_objects to num_varaints: {len(vrs_objects)}/{num_variants}={(len(vrs_objects)/num_variants):.2f}%')\n",
    "    except:\n",
    "        print(\"unable to get pickle totals, file may not exist\")\n",
    "    print()\n",
    "\n",
    "print(f\"total successes: {len(successes)}/{len(drs_vcfs)} \\nList...\")\n",
    "for vcf_path in drs_vcfs:\n",
    "    print(f\"{vcf_path}: {'✓' if vcf_path in successes else 'x'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import vcf\n",
    "\n",
    "\n",
    "# for input_vcf_file in [\"/home/jupyter/vcf/long_read_sv_jasmine_Trios_IndividualCallsets_CHM13_HG005_Trio_HG006vCHM13_20200921_mm2_PBCCS_sniffles.s2l20.refined.nSVtypes.ism.vcf\"]:\n",
    "for input_vcf_file in [\"/home/jupyter/vcf/long_read_minimap2_alignments_HG02080vCHM13_20200921_mm2_ONT_sniffles.s2l20.refined.nSVtypes.ism.vcf\"]:\n",
    "    output_vcf_file = \"/home/jupyter/vcf/long_read.test.vcf\"\n",
    "\n",
    "    vcf_reader = vcf.Reader(open(input_vcf_file, 'r'))\n",
    "    vcf_writer = vcf.Writer(open(output_vcf_file, 'w'), vcf_reader)\n",
    "\n",
    "    for record in vcf_reader:\n",
    "        record.INFO['VRS_ALLELE_ID'] = 'ga4gh:VA.xksahgfowdfdwofd,ga4gh:VA.xksahgfowdfdwofd'\n",
    "        vcf_writer.write_record(record)\n",
    "\n",
    "vcf_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### show loaded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# import pickle\n",
    "# import ast\n",
    "# import requests\n",
    "# import datetime\n",
    "\n",
    "# # log progress\n",
    "# progress_interval = 50000\n",
    "\n",
    "# # load pickled dict\n",
    "# with open(output_pkl, 'rb') as f:\n",
    "#     print(datetime.datetime.now().isoformat(), 'opened pickle')\n",
    "#     vrs_objects = pickle.load(f)\n",
    "#     c = 0\n",
    "#     for k, v in vrs_objects.items():\n",
    "#         vrs_objects[k] = ast.literal_eval(v)\n",
    "#         c += 1\n",
    "#         if c % progress_interval == 0:\n",
    "#             print(datetime.datetime.now().isoformat(), c)\n",
    "\n",
    "# # view details        \n",
    "# print('number of vrs objects', len(vrs_objects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pickle_paths = !ls -1 ~/split/*.vcf.pkl\n",
    "pickle_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get percent of loaded variants\n",
    "\n",
    "# load pickled dict\n",
    "# for vcf_path in drs_vcfs:\n",
    "\n",
    "def unpickle_generator(file_name):\n",
    "    \"\"\"Unpickle vrs objects, yields (key,vrs_object)\"\"\"\n",
    "    with open(file_name, 'rb') as f:\n",
    "        vrs_objects = pickle.load(f)\n",
    "        for k, v in vrs_objects.items():\n",
    "            yield k, ast.literal_eval(v)\n",
    "            \n",
    "def unpickle(file_name):\n",
    "    \"\"\"Unpickle vrs objects to single dict\"\"\"\n",
    "    with open(file_name, 'rb') as f:\n",
    "        vrs_objects = pickle.load(f)\n",
    "        for k, v in vrs_objects.items():\n",
    "            vrs_objects[k] = ast.literal_eval(v)\n",
    "    \n",
    "    return vrs_objects\n",
    "\n",
    "vrs_dicts = []\n",
    "\n",
    "total_num_vrs_objs = 0\n",
    "\n",
    "for path in pickle_paths:\n",
    "    vrs_dict = unpickle(path)\n",
    "    vrs_dicts.append(vrs_dict)\n",
    "\n",
    "    # get total num_variants\n",
    "    # TODO: reference the new merged file bc some might have been filtered out\n",
    "    vcf_reader = vcf.Reader(open(path[:-4], 'r'))\n",
    "    num_variants = sum(1 for record in vcf_reader)\n",
    "\n",
    "#     num_vrs_objs = sum((1 for _ in vrs_objects))\n",
    "    num_vrs_objs = len(vrs_dict)\n",
    "    total_num_vrs_objs += num_vrs_objs\n",
    "\n",
    "    # view details\n",
    "    \n",
    "    print(path.split(\"/\")[-1], end=\" \")\n",
    "    if num_variants == 0: \n",
    "        print(f\"no variants\") \n",
    "    else:\n",
    "        print(f'vrs_objects:variants = {num_vrs_objs}/{num_variants} = {(50*num_vrs_objs/num_variants):.1f}%')\n",
    "\n",
    "total_variants = get_num_variants(vcf_path)\n",
    "        \n",
    "print(f\"Totals: {total_num_vrs_objs}/{total_variants}\", \\\n",
    "      f\"= {(50*total_num_vrs_objs/total_variants):.2f}%\")\n",
    "        \n",
    "# TODO on combining: have to think about this more bc large files will have to be held in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# error reporting from logs\n",
    "num_val_errors = !(grep \"raise ValidationError(err_msg)\" $HOME/log.txt | wc -l)\n",
    "num_val_errors = int(num_val_errors[0])\n",
    "print(f\"validations errors = {num_val_errors}, ie {50*num_val_errors/total_variants:.1f}%\", \\\n",
    "      \" if 2:1 VRS ID to variant\")\n",
    "\n",
    "num_invalid_files = !(grep \"\\[E::vcf_format\\] Invalid BCF\" $HOME/log.txt | wc -l)\n",
    "num_invalid_files = int(num_invalid_files[0])\n",
    "print(f\"num invalid files: {num_invalid_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query remote services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MetaKB (cancervariants.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "from biocommons.seqrepo import SeqRepo\n",
    "from ga4gh.core import ga4gh_identify\n",
    "from ga4gh.vrs import models\n",
    "from ga4gh.vrs.dataproxy import SeqRepoDataProxy\n",
    "from ga4gh.vrs.extras.translator import AlleleTranslator\n",
    "\n",
    "\n",
    "def meta_kb(item: tuple, translator=None, fmt=None, recent=True):\n",
    "    \"\"\"Query metakb using vrs object\"\"\"\n",
    "    k, allele_dict = item\n",
    "    \n",
    "    if translator is not None:\n",
    "        print(f\"by {fmt}...\")\n",
    "#         print(json.dumps(allele_dict, indent=2))\n",
    "        \n",
    "        allele = models.Allele(**allele_dict)\n",
    "        _id = translator.translate_to(allele, fmt)\n",
    "        \n",
    "#         seq_ref = models.SequenceReference(**allele_dict[\"sequenceReference\"])\n",
    "#         location = models.SequenceLocation(**allele_dict[\"location\"])\n",
    "#         state = models.LiteralSequenceExpression(sequence=ins_seq)\n",
    "#         allele = models.Allele(location=location, state=state)\n",
    "#         allele = self._post_process_imported_allele(allele)\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"by vrs id...\")\n",
    "        _id = allele_dict[\"id\"]\n",
    "        \n",
    "        \n",
    "    if recent:\n",
    "        print(\"recent elasticbeanstalk api (VRS 2.0 models)\")\n",
    "        response = requests.get(\"http://metakb-dev-eb.us-east-2.elasticbeanstalk.com\",\n",
    "                                f\"/api/v2/search/studies?variation={_id}&detail=false\")\n",
    "    else:\n",
    "        print(\"old api (VRS 1.3 models)\")\n",
    "        response = requests.get(\"https://dev-search.cancervariants.org\" \\\n",
    "                                f\"/api/v2/search?variation={_id}&detail=false\")\n",
    "    \n",
    "    response_json = response.json()\n",
    "    \n",
    "    if response_json['warnings'] == []:\n",
    "        summary = {}\n",
    "        summary['description'] = response_json['statements'][0]['description']\n",
    "        return (k, _id, summary)\n",
    "\n",
    "#########################\n",
    "# setup translator\n",
    "seqrepo_root_dir = f\"{os.environ['SEQREPO_ROOT']}/latest\"\n",
    "data_proxy = SeqRepoDataProxy(SeqRepo(seqrepo_root_dir))\n",
    "translator = AlleleTranslator(data_proxy)\n",
    "        \n",
    "for vrs_dict in vrs_dicts:\n",
    "    hits = []\n",
    "    \n",
    "    for obj in vrs_dict.items(): \n",
    "        try:\n",
    "            potential_hit = meta_kb(obj, translator, fmt=\"hgvs\")\n",
    "        except:\n",
    "            continue\n",
    "        if potential_hit:\n",
    "            print(f\"\\n ~~~~~~~~ hit! {potential_hit} ~~~~~~~~~~ \\n\")\n",
    "            hits.append(potential_hit)\n",
    "    \n",
    "    if len(vrs_dict) == 0:\n",
    "        continue\n",
    "\n",
    "    hit_rate = len(hits)/len(vrs_dict)    \n",
    "    print(f\"hit rate of VRS IDs: {len(hits)}/{len(vrs_dict)}={100*hit_rate:.1f}%\" )\n",
    "    if len(hits) > 0:\n",
    "        print(\"first few hits\")\n",
    "        print(hits[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for vrs_dict in vrs_dicts[:1]:\n",
    "    hits = []\n",
    "    \n",
    "    for i, (k, v) in enumerate(vrs_dict.items()):\n",
    "        print(v[\"id\"])\n",
    "        if i > 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def meta_kb_by_sequence(item: tuple):\n",
    "    \"\"\"Query metakb\"\"\"\n",
    "    k, _ = item\n",
    "    \n",
    "\n",
    "    response = requests.get(f\"https://dev-search.cancervariants.org/api/v2/search?variation={}&detail=false\")\n",
    "    response_json = response.json()\n",
    "    \n",
    "    print(response_json['warnings'])\n",
    "    if response_json['warnings'] == []:\n",
    "        summary = {}\n",
    "        summary['description'] = response_json['statements'][0]['description']\n",
    "        return (k, _['_id'], summary)\n",
    "\n",
    "print(\"trying old link\")\n",
    "for vrs_dict in vrs_dicts:\n",
    "    hits = []\n",
    "    \n",
    "    for obj in vrs_dict.items():\n",
    "        potential_hit = meta_kb_by_sequence(obj)\n",
    "        if potential_hit:\n",
    "            hits.append(potential_hit)\n",
    "    \n",
    "    if len(vrs_dict) == 0:\n",
    "        print(\"no variants, skipping...\")\n",
    "        continue\n",
    "\n",
    "    hit_rate = len(hits)/len(vrs_dict)    \n",
    "    print(f\"hit rate of VRS IDs: {len(hits)}/{len(vrs_dict)}={100*hit_rate:.1f}%\" )\n",
    "    if len(hits) > 0:\n",
    "        print(\"first few hits\")\n",
    "        print(hits[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def decorate(vrs_decorator, vrs_objects, limit=20):\n",
    "    \"\"\"harvest data from service\"\"\"\n",
    "\n",
    "    # log progress\n",
    "    progress_interval = 1000\n",
    "\n",
    "\n",
    "    # number of workers\n",
    "    worker_count = 12\n",
    "\n",
    "    with multiprocessing.Pool(worker_count) as pool:\n",
    "        # call the function for each item in parallel\n",
    "        c = 0\n",
    "        print(datetime.datetime.now().isoformat(), c)\n",
    "        for result in pool.imap(vrs_decorator, vrs_objects.items()):\n",
    "            c += 1\n",
    "            if result:\n",
    "                print(result[0], result[-1])\n",
    "            if c == limit:\n",
    "                break\n",
    "            if c % progress_interval == 0:\n",
    "                print(datetime.datetime.now().isoformat(), c)\n",
    "\n",
    "    print(datetime.datetime.now().isoformat(), c)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "decorate(vrs_decorator=meta_kb, vrs_objects=metakb_vrs_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ClinGen (clinicalgenome.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clingen(item: tuple):\n",
    "    \"\"\"Query clingen (old version of normalizer)\"\"\"\n",
    "    k, _ = item\n",
    "    _id = _['_id'].split(':')[-1].split('.')[-1]\n",
    "    response = requests.get(f\"https://reg.genome.network/vrs-map/digest/vrs/{_id}\")\n",
    "    response_json = response.json()\n",
    "    if response_json['status']['code'] == 200:\n",
    "        iri_response = requests.get(response_json['data']['iri'])\n",
    "        iri_response_json = iri_response.json()\n",
    "        return (k, _['_id'], {'communityStandardTitle': iri_response_json['communityStandardTitle']})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: at this time, there is a schema mismatch between vrs-python, metakb and clingen. We will use known identifiers. Normally the annotated identifiers from the variants of interest (vcf) would be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "decorate(vrs_decorator=clingen, vrs_objects=clingen_vrs_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
